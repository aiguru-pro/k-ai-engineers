{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9968bc741a2f46daab66a25ed12eadaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb0f24dd633d4ba0a0ff3164d3845fe5",
              "IPY_MODEL_59b017de9ebb47bfaf575ba753001ed2",
              "IPY_MODEL_1afb3f59d8da4fbfab7edd14530d40c5"
            ],
            "layout": "IPY_MODEL_967be5f0f1ab4e488022a983398a1d92"
          }
        },
        "bb0f24dd633d4ba0a0ff3164d3845fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_407faeb8c351416db418960bcd5400d3",
            "placeholder": "​",
            "style": "IPY_MODEL_aa7111b98b7a4d4bb5ff7f8b6a791c4c",
            "value": "Generating embeddings: 100%"
          }
        },
        "59b017de9ebb47bfaf575ba753001ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36f9f0d0559a47c28d65be6743f74759",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_167797ead34247ebaeddc982b8eb7f01",
            "value": 10
          }
        },
        "1afb3f59d8da4fbfab7edd14530d40c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91541be8b9fb44a7a61fa5770975ab58",
            "placeholder": "​",
            "style": "IPY_MODEL_8ac654315f7a486ab28fdd882a3b2fad",
            "value": " 10/10 [00:03&lt;00:00,  3.07it/s]"
          }
        },
        "967be5f0f1ab4e488022a983398a1d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "407faeb8c351416db418960bcd5400d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7111b98b7a4d4bb5ff7f8b6a791c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36f9f0d0559a47c28d65be6743f74759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "167797ead34247ebaeddc982b8eb7f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91541be8b9fb44a7a61fa5770975ab58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac654315f7a486ab28fdd882a3b2fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG with LlamaIndex - Medical Literature Demo"
      ],
      "metadata": {
        "id": "8a5cY_5WfaIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installs all necessary libraries for our medical RAG system:\n",
        "- **llama-index**: Core framework for building RAG applications\n",
        "- **llama-index-embeddings-huggingface**: To use medical-specific embedding models\n",
        "- **llama-index-llms-openai**: For GPT integration\n",
        "- **llama-index-vector-stores-qdrant**: Qdrant vector database integration\n",
        "- **sentence-transformers**: For creating embeddings\n",
        "- **ragatouille**: For ColBERT reranking (improves retrieval quality)\n",
        "- **qdrant-client**: Client for Qdrant vector database\n",
        "\n",
        "We're using Qdrant as our vector store because:\n",
        "1. It runs locally in Colab (no external dependencies)\n",
        "2. Optimized for vector similarity search\n",
        "3. Supports metadata filtering\n",
        "4. Better performance than in-memory stores for larger datasets\n"
      ],
      "metadata": {
        "id": "d2_r0kenfgHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index llama-index-embeddings-huggingface llama-index-llms-openai\n",
        "!pip install -q llama-index-vector-stores-qdrant qdrant-client\n",
        "!pip install -q pypdf arxiv pubmed-parser biopython\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q gradio plotly pandas\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "jQi6NCkXfsCN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index==0.10.57 llama-index-vector-stores-qdrant\n",
        "! pip show llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0QvhM31mNO3",
        "outputId": "53f18101-5471-4c96-ee81-8287f5336cd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index==0.10.57 in /usr/local/lib/python3.11/dist-packages (0.10.57)\n",
            "Requirement already satisfied: llama-index-vector-stores-qdrant in /usr/local/lib/python3.11/dist-packages (0.2.17)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.2.9)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.13)\n",
            "Requirement already satisfied: llama-index-core==0.10.57 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.10.57)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.11)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.2.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.9.48.post4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.31)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.9)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.7)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.33)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from llama-index==0.10.57) (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.57->llama-index==0.10.57) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (3.5)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (1.86.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (11.2.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core==0.10.57->llama-index==0.10.57) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (1.73.0)\n",
            "Requirement already satisfied: qdrant-client>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (1.14.3)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.57) (0.1.26)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.57) (4.13.4)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.57) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.57) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.57) (0.4.9)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.11.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.57->llama-index==0.10.57) (1.20.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.57) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core==0.10.57->llama-index==0.10.57) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core==0.10.57->llama-index==0.10.57) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core==0.10.57->llama-index==0.10.57) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core==0.10.57->llama-index==0.10.57) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.57->llama-index==0.10.57) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (4.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.57->llama-index==0.10.57) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.57->llama-index==0.10.57) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.57->llama-index==0.10.57) (2024.11.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-core==0.10.57->llama-index==0.10.57) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-core==0.10.57->llama-index==0.10.57) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-core==0.10.57->llama-index==0.10.57) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core==0.10.57->llama-index==0.10.57) (3.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.57->llama-index==0.10.57) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.57->llama-index==0.10.57) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core==0.10.57->llama-index==0.10.57) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-core==0.10.57->llama-index==0.10.57) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-core==0.10.57->llama-index==0.10.57) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-core==0.10.57->llama-index==0.10.57) (2025.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (4.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.57->llama-index==0.10.57) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.57->llama-index==0.10.57) (1.17.0)\n",
            "Name: llama-index\n",
            "Version: 0.10.57\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: Jerry Liu\n",
            "Author-email: jerry@llamaindex.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Securely collects your OpenAI API key using getpass (hides input).\n",
        "The API key is needed for:\n",
        "- Using GPT models for answer generation\n",
        "- Creating hypothetical documents (HyDE technique)\n",
        "- Query understanding and routing\n",
        "\n",
        "Using getpass ensures your API key isn't visible in the notebook.\n"
      ],
      "metadata": {
        "id": "MfZVx5ELftoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "from getpass import getpass\n",
        "openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAMunnMRf8BK",
        "outputId": "b7616764-f617-4b95-9a16-e8edbb925130"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates sample medical research papers for our demo. In production, you'd load real papers.\n",
        "\n",
        "The papers cover:\n",
        "1. **AI in Diabetes**: Shows how AI improves patient outcomes\n",
        "2. **COVID Vaccine Effectiveness**: Real-world vaccine performance data\n",
        "3. **Digital Mental Health**: Efficacy of app-based interventions\n",
        "\n",
        "Each paper has standard sections (Abstract, Methods, Results, etc.) which we'll use\n",
        "for section-aware chunking - a key technique for better retrieval from structured documents."
      ],
      "metadata": {
        "id": "5YxGg3v3f-qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create data directory\n",
        "Path(\"./medical_papers\").mkdir(exist_ok=True)\n",
        "\n",
        "# Sample medical papers (using PubMed Central open access papers)\n",
        "sample_papers = [\n",
        "    {\n",
        "        \"title\": \"diabetes_management_ai.txt\",\n",
        "        \"content\": \"\"\"\n",
        "        Title: AI-Driven Approaches in Diabetes Management: A Systematic Review\n",
        "\n",
        "        Abstract: This systematic review examines the application of artificial intelligence in diabetes management,\n",
        "        analyzing 127 studies from 2018-2023. Key findings include improved glycemic control through predictive\n",
        "        algorithms (mean HbA1c reduction of 0.8%), enhanced patient engagement via chatbots (42% increase in\n",
        "        medication adherence), and early complication detection using retinal imaging analysis (sensitivity 94.2%).\n",
        "\n",
        "        Methods: We searched PubMed, MEDLINE, and IEEE Xplore for studies implementing AI solutions in diabetes care.\n",
        "        Inclusion criteria required randomized controlled trials or cohort studies with minimum 6-month follow-up.\n",
        "        Machine learning approaches were categorized into supervised learning for glucose prediction, reinforcement\n",
        "        learning for insulin dosing, and deep learning for complication screening.\n",
        "\n",
        "        Results: Continuous glucose monitoring (CGM) data combined with neural networks achieved 87% accuracy in\n",
        "        predicting hypoglycemic events 30 minutes in advance. Mobile applications using natural language processing\n",
        "        showed significant improvements in dietary logging compliance (68% vs 23% traditional methods). Computer\n",
        "        vision analysis of fundus photographs detected diabetic retinopathy with comparable accuracy to specialists.\n",
        "\n",
        "        Conclusion: AI technologies demonstrate substantial promise in personalizing diabetes management. However,\n",
        "        challenges remain in data standardization, algorithm interpretability, and equitable access across diverse\n",
        "        populations. Future research should focus on prospective validation and real-world implementation studies.\n",
        "        \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"covid19_vaccine_effectiveness.txt\",\n",
        "        \"content\": \"\"\"\n",
        "        Title: Real-World Effectiveness of COVID-19 Vaccines: A Multi-Country Analysis\n",
        "\n",
        "        Abstract: This observational study analyzed vaccine effectiveness across 8 countries covering 125 million\n",
        "        individuals. Primary outcomes included infection prevention, hospitalization rates, and severe disease.\n",
        "        Two-dose mRNA vaccines showed 91% effectiveness against hospitalization, declining to 78% after 6 months.\n",
        "        Booster doses restored effectiveness to 94%.\n",
        "\n",
        "        Introduction: The rapid development of COVID-19 vaccines necessitated ongoing real-world effectiveness\n",
        "        monitoring. This study leveraged electronic health records from participating healthcare systems to assess\n",
        "        vaccine performance across diverse populations and viral variants.\n",
        "\n",
        "        Methods: Retrospective cohort analysis using propensity score matching compared vaccinated and unvaccinated\n",
        "        individuals. Cox proportional hazards models adjusted for age, comorbidities, and socioeconomic factors.\n",
        "        Variant-specific analyses used genomic surveillance data. Waning immunity assessed through time-varying\n",
        "        effectiveness calculations.\n",
        "\n",
        "        Results: Among fully vaccinated individuals, breakthrough infection rate was 3.2 per 1000 person-months.\n",
        "        Vaccine effectiveness against Delta variant: 88% for infection, 96% for severe disease. Omicron variant\n",
        "        showed reduced effectiveness: 64% for infection, 89% for severe disease. Age-stratified analysis revealed\n",
        "        lower effectiveness in >75 years (82% vs 93% in 18-64 years). Immunocompromised populations showed\n",
        "        significantly reduced responses (effectiveness 71%).\n",
        "\n",
        "        Discussion: Findings support continued booster recommendations, especially for vulnerable populations.\n",
        "        Variant-specific effectiveness highlights importance of updated vaccine formulations. Study limitations\n",
        "        include potential unmeasured confounders and varying testing practices across regions.\n",
        "        \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"mental_health_digital_interventions.txt\",\n",
        "        \"content\": \"\"\"\n",
        "        Title: Digital Mental Health Interventions: Efficacy and Implementation Challenges\n",
        "\n",
        "        Abstract: Meta-analysis of 89 randomized controlled trials (n=15,492) examining digital mental health\n",
        "        interventions. Cognitive behavioral therapy apps showed moderate effect sizes for depression (d=0.54)\n",
        "        and anxiety (d=0.48). Engagement remained primary challenge with 68% dropout rates by week 8.\n",
        "\n",
        "        Background: Rising mental health needs and provider shortages drive interest in scalable digital solutions.\n",
        "        This review synthesizes evidence on smartphone apps, web platforms, and virtual reality interventions for\n",
        "        common mental health conditions.\n",
        "\n",
        "        Methodology: Systematic search of databases through December 2023. Included studies required validated\n",
        "        clinical assessments, minimum 4-week interventions, and control groups. Random-effects models calculated\n",
        "        pooled effect sizes. Subgroup analyses examined modality, condition, and user characteristics.\n",
        "\n",
        "        Key Findings: Self-guided interventions showed smaller effects than therapist-supported programs (d=0.31 vs\n",
        "        d=0.76). Gamification elements improved engagement by 34%. Younger users (<30 years) demonstrated better\n",
        "        outcomes. Virtual reality exposure therapy for phobias achieved large effect sizes (d=1.12). Text-based\n",
        "        chatbots reduced suicidal ideation in crisis situations (OR=0.64). However, cultural adaptation remained\n",
        "        limited with 78% of studies from Western populations.\n",
        "\n",
        "        Implementation Barriers: Privacy concerns affected 43% of potential users. Integration with existing care\n",
        "        systems proved challenging. Clinician training needs identified as critical factor. Reimbursement models\n",
        "        lagged behind technology development.\n",
        "\n",
        "        Recommendations: Future interventions should prioritize user engagement strategies, cultural competence,\n",
        "        and seamless clinical integration. Regulatory frameworks need updating to ensure quality while enabling\n",
        "        innovation. Long-term effectiveness studies beyond 6 months urgently needed.\n",
        "        \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save sample papers\n",
        "for paper in sample_papers:\n",
        "    with open(f\"./medical_papers/{paper['title']}\", 'w') as f:\n",
        "        f.write(paper['content'])\n",
        "\n",
        "print(\"✓ Sample medical papers created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuw6yhLJgFiN",
        "outputId": "c89b756e-b652-4e21-bb6b-067567249ab2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Sample medical papers created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sets up the core components of our RAG system:\n",
        "\n",
        "1. **Embedding Model**: Uses BioBERT (medical-specific) instead of generic embeddings\n",
        "   - Better understands medical terminology\n",
        "   - Trained on PubMed papers and clinical notes\n",
        "   \n",
        "2. **LLM Configuration**: GPT-3.5 with low temperature for factual accuracy\n",
        "\n",
        "3. **Document Loading**: Reads papers and extracts metadata (title, type)\n",
        "\n",
        "The embedding model is crucial - medical terms like \"myocardial infarction\" and\n",
        "\"heart attack\" should have similar embeddings, which generic models might miss."
      ],
      "metadata": {
        "id": "SuKagI92gLU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document, VectorStoreIndex, ServiceContext\n",
        "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "import re\n",
        "\n",
        "# Initialize embedding model (using medical-specialized model)\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
        "    cache_folder=\"./embedding_cache\"\n",
        ")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "\n",
        "# Configure global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "# Load documents\n",
        "documents = []\n",
        "for filename in Path(\"./medical_papers\").glob(\"*.txt\"):\n",
        "    with open(filename, 'r') as f:\n",
        "        content = f.read()\n",
        "        # Extract metadata from content\n",
        "        title_match = re.search(r'Title: (.+)', content)\n",
        "        title = title_match.group(1) if title_match else filename.stem\n",
        "\n",
        "        doc = Document(\n",
        "            text=content,\n",
        "            metadata={\n",
        "                \"filename\": filename.name,\n",
        "                \"title\": title,\n",
        "                \"doc_type\": \"research_paper\"\n",
        "            }\n",
        "        )\n",
        "        documents.append(doc)\n",
        "\n",
        "print(f\"✓ Loaded {len(documents)} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDHEetLFgQ3h",
        "outputId": "5316e2cd-5d4e-433e-94c5-051aeeb0b83f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded 3 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements two chunking strategies:\n",
        "\n",
        "1. **Semantic Chunking**: Splits text at natural meaning boundaries\n",
        "   - Uses embedding similarity to find good split points\n",
        "   - Prevents breaking up related concepts\n",
        "   \n",
        "2. **Section-Aware Chunking**: Preserves paper structure\n",
        "   - Keeps Abstract, Methods, Results separate\n",
        "   - Maintains context within sections\n",
        "   - Adds section metadata for better filtering\n",
        "\n",
        "Why this matters: A chunk about \"vaccine effectiveness\" from the Results section\n",
        "is different from one in the Methods section. Section-aware chunking preserves\n",
        "this context for more accurate retrieval.\n"
      ],
      "metadata": {
        "id": "Ut3b1qtogUKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Semantic chunking (preserves meaning boundaries)\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "\n",
        "semantic_splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1,  # sentences to include before/after for context\n",
        "    breakpoint_percentile_threshold=95,  # higher = fewer splits\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "# Method 2: Section-aware chunking for medical papers\n",
        "class MedicalPaperParser:\n",
        "    def __init__(self, chunk_size=512, chunk_overlap=50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.sections = ['Abstract', 'Introduction', 'Methods', 'Results', 'Discussion', 'Conclusion']\n",
        "\n",
        "    def parse(self, document):\n",
        "        text = document.text\n",
        "        nodes = []\n",
        "\n",
        "        # Extract sections\n",
        "        for i, section in enumerate(self.sections):\n",
        "            pattern = rf'{section}:?\\s*(.+?)(?={\"|\".join(self.sections[i+1:])}:|$)'\n",
        "            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "            if match:\n",
        "                section_text = match.group(1).strip()\n",
        "                # Create chunks within sections\n",
        "                splitter = SentenceSplitter(\n",
        "                    chunk_size=self.chunk_size,\n",
        "                    chunk_overlap=self.chunk_overlap\n",
        "                )\n",
        "                section_nodes = splitter.get_nodes_from_documents(\n",
        "                    [Document(text=section_text)]\n",
        "                )\n",
        "\n",
        "                # Add section metadata\n",
        "                for node in section_nodes:\n",
        "                    node.metadata.update({\n",
        "                        \"section\": section,\n",
        "                        \"filename\": document.metadata.get(\"filename\"),\n",
        "                        \"title\": document.metadata.get(\"title\")\n",
        "                    })\n",
        "                nodes.extend(section_nodes)\n",
        "\n",
        "        return nodes\n",
        "\n",
        "# Apply both chunking methods\n",
        "medical_parser = MedicalPaperParser()\n",
        "all_nodes = []\n",
        "\n",
        "for doc in documents:\n",
        "    # Get section-aware chunks\n",
        "    section_nodes = medical_parser.parse(doc)\n",
        "    all_nodes.extend(section_nodes)\n",
        "\n",
        "print(f\"✓ Created {len(all_nodes)} chunks using section-aware parsing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aI6AWADgbhN",
        "outputId": "46f98da2-feb6-4544-ddfc-603c0193457a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created 10 chunks using section-aware parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates our vector database using Qdrant and implements hybrid search.\n",
        "\n",
        "**Why Qdrant?**\n",
        "- Runs entirely in-memory for Colab (no external server needed)\n",
        "- Production-ready with same API\n",
        "- Supports metadata filtering\n",
        "- Better performance than default in-memory store\n",
        "\n",
        "**Hybrid Search combines:**\n",
        "1. Vector search (semantic similarity)\n",
        "2. BM25 (keyword matching)\n",
        "\n",
        "This is crucial for medical queries where users might search for:\n",
        "- Specific drug names (keyword match works better)\n",
        "- Conceptual questions (vector search works better)\n"
      ],
      "metadata": {
        "id": "Zq4u6q7pgess"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip uninstall -y llama-index llama-index-core llama-index-embeddings-openai llama-index-llms-openai llama-index-readers-file llama-index-readers-llama-parse llama-index-cli llama-index-program-openai"
      ],
      "metadata": {
        "id": "qz2whkhnkNPh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip list | grep llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DGbirNLmueC",
        "outputId": "36e90ff2-27d5-4613-cfb3-ef92682fe777"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-cloud                             0.1.26\n",
            "llama-cloud-services                    0.6.34\n",
            "llama-index                             0.10.57\n",
            "llama-index-agent-openai                0.2.9\n",
            "llama-index-cli                         0.1.13\n",
            "llama-index-core                        0.10.57\n",
            "llama-index-embeddings-huggingface      0.2.3\n",
            "llama-index-embeddings-openai           0.1.11\n",
            "llama-index-indices-managed-llama-cloud 0.2.7\n",
            "llama-index-instrumentation             0.2.0\n",
            "llama-index-legacy                      0.9.48.post4\n",
            "llama-index-llms-openai                 0.1.31\n",
            "llama-index-multi-modal-llms-openai     0.1.9\n",
            "llama-index-program-openai              0.1.7\n",
            "llama-index-question-gen-openai         0.1.3\n",
            "llama-index-readers-file                0.1.33\n",
            "llama-index-readers-llama-parse         0.1.6\n",
            "llama-index-vector-stores-qdrant        0.2.17\n",
            "llama-index-workflows                   0.2.2\n",
            "llama-parse                             0.4.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip show llama-index\n",
        "from llama_index.core import (\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "# Fixed import - BM25Retriever is now in core.retrievers\n",
        "from llama_index.core.retrievers import VectorIndexRetriever, BaseRetriever\n",
        "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Qdrant in-memory\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Create collection with proper vector size\n",
        "vector_size = len(embed_model.get_text_embedding(\"test\"))\n",
        "client.create_collection(\n",
        "    collection_name=\"medical_papers\",\n",
        "    vectors_config=VectorParams(\n",
        "        size=vector_size,\n",
        "        distance=Distance.COSINE\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create Qdrant vector store\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"medical_papers\"\n",
        ")\n",
        "\n",
        "# Create storage context with Qdrant\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "# Create vector index with Qdrant backend\n",
        "vector_index = VectorStoreIndex(\n",
        "    nodes=all_nodes,\n",
        "    storage_context=storage_context,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "# Simple BM25-like retriever implementation since the module import is not available\n",
        "class SimpleBM25Retriever(BaseRetriever):\n",
        "    \"\"\"Simple BM25-like retriever implementation\"\"\"\n",
        "\n",
        "    def __init__(self, nodes, similarity_top_k=10):\n",
        "        self.nodes = nodes\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "        super().__init__()\n",
        "\n",
        "        # Build simple term frequency index\n",
        "        self._build_tf_index()\n",
        "\n",
        "    def _build_tf_index(self):\n",
        "        \"\"\"Build term frequency index for all nodes\"\"\"\n",
        "        import re\n",
        "        from collections import defaultdict, Counter\n",
        "\n",
        "        self.doc_term_freqs = []\n",
        "        self.doc_lengths = []\n",
        "        all_terms = set()\n",
        "\n",
        "        for node in self.nodes:\n",
        "            # Simple tokenization\n",
        "            text = node.text.lower()\n",
        "            terms = re.findall(r'\\b\\w+\\b', text)\n",
        "            term_freq = Counter(terms)\n",
        "\n",
        "            self.doc_term_freqs.append(term_freq)\n",
        "            self.doc_lengths.append(len(terms))\n",
        "            all_terms.update(terms)\n",
        "\n",
        "        self.vocab = list(all_terms)\n",
        "\n",
        "        # Calculate document frequencies\n",
        "        self.doc_freqs = defaultdict(int)\n",
        "        for term_freq in self.doc_term_freqs:\n",
        "            for term in term_freq:\n",
        "                self.doc_freqs[term] += 1\n",
        "\n",
        "    def _bm25_score(self, query_terms, doc_idx, k1=1.2, b=0.75):\n",
        "        \"\"\"Calculate BM25 score for a document\"\"\"\n",
        "        import math\n",
        "\n",
        "        score = 0.0\n",
        "        doc_tf = self.doc_term_freqs[doc_idx]\n",
        "        doc_len = self.doc_lengths[doc_idx]\n",
        "        avg_doc_len = sum(self.doc_lengths) / len(self.doc_lengths)\n",
        "        N = len(self.nodes)\n",
        "\n",
        "        for term in query_terms:\n",
        "            if term in doc_tf:\n",
        "                tf = doc_tf[term]\n",
        "                df = self.doc_freqs[term]\n",
        "\n",
        "                # BM25 formula\n",
        "                idf = math.log((N - df + 0.5) / (df + 0.5))\n",
        "                score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _retrieve(self, query_bundle):\n",
        "        import re\n",
        "\n",
        "        # Tokenize query\n",
        "        query_terms = re.findall(r'\\b\\w+\\b', query_bundle.query_str.lower())\n",
        "\n",
        "        # Score all documents\n",
        "        scores = []\n",
        "        for i, node in enumerate(self.nodes):\n",
        "            score = self._bm25_score(query_terms, i)\n",
        "            scores.append((node, score))\n",
        "\n",
        "        # Sort by score and return top k\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [NodeWithScore(node=node, score=score)\n",
        "                for node, score in scores[:self.similarity_top_k]]\n",
        "\n",
        "# Create BM25-like retriever using our implementation\n",
        "bm25_retriever = SimpleBM25Retriever(\n",
        "    nodes=all_nodes,\n",
        "    similarity_top_k=10\n",
        ")\n",
        "\n",
        "# Implement hybrid retriever\n",
        "class HybridRetriever(BaseRetriever):\n",
        "    def __init__(self, vector_retriever, bm25_retriever, alpha=0.5):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.bm25_retriever = bm25_retriever\n",
        "        self.alpha = alpha  # weight for vector search (1-alpha for BM25)\n",
        "        super().__init__()\n",
        "\n",
        "    def _retrieve(self, query_bundle):\n",
        "        # Get results from both retrievers\n",
        "        vector_results = self.vector_retriever.retrieve(query_bundle)\n",
        "        bm25_results = self.bm25_retriever.retrieve(query_bundle)\n",
        "\n",
        "        # Normalize scores to 0-1 range\n",
        "        def normalize_scores(results):\n",
        "            if not results:\n",
        "                return results\n",
        "            scores = [r.score for r in results]\n",
        "            max_score = max(scores) if scores else 1.0\n",
        "            min_score = min(scores) if scores else 0.0\n",
        "            score_range = max_score - min_score if max_score != min_score else 1.0\n",
        "\n",
        "            for result in results:\n",
        "                result.score = (result.score - min_score) / score_range\n",
        "            return results\n",
        "\n",
        "        # Normalize scores\n",
        "        vector_results = normalize_scores(vector_results)\n",
        "        bm25_results = normalize_scores(bm25_results)\n",
        "\n",
        "        # Combine scores\n",
        "        all_nodes = {}\n",
        "\n",
        "        # Add vector search results\n",
        "        for node in vector_results:\n",
        "            all_nodes[node.node.node_id] = {\n",
        "                'node': node.node,\n",
        "                'vector_score': node.score,\n",
        "                'bm25_score': 0.0\n",
        "            }\n",
        "\n",
        "        # Add BM25 results\n",
        "        for node in bm25_results:\n",
        "            if node.node.node_id in all_nodes:\n",
        "                all_nodes[node.node.node_id]['bm25_score'] = node.score\n",
        "            else:\n",
        "                all_nodes[node.node.node_id] = {\n",
        "                    'node': node.node,\n",
        "                    'vector_score': 0.0,\n",
        "                    'bm25_score': node.score\n",
        "                }\n",
        "\n",
        "        # Calculate hybrid scores\n",
        "        hybrid_results = []\n",
        "        for node_id, data in all_nodes.items():\n",
        "            # Calculate weighted hybrid score\n",
        "            hybrid_score = (self.alpha * data['vector_score'] +\n",
        "                          (1 - self.alpha) * data['bm25_score'])\n",
        "            hybrid_results.append((data['node'], hybrid_score))\n",
        "\n",
        "        # Sort by score and return top k\n",
        "        hybrid_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [NodeWithScore(node=node, score=score)\n",
        "                for node, score in hybrid_results[:10]]\n",
        "\n",
        "# Create retrievers\n",
        "vector_retriever = VectorIndexRetriever(\n",
        "    index=vector_index,\n",
        "    similarity_top_k=10,\n",
        ")\n",
        "\n",
        "hybrid_retriever = HybridRetriever(\n",
        "    vector_retriever=vector_retriever,\n",
        "    bm25_retriever=bm25_retriever,\n",
        "    alpha=0.7  # Favor semantic search\n",
        ")\n",
        "\n",
        "print(\"✓ Built hybrid search index with custom BM25 implementation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "9968bc741a2f46daab66a25ed12eadaa",
            "bb0f24dd633d4ba0a0ff3164d3845fe5",
            "59b017de9ebb47bfaf575ba753001ed2",
            "1afb3f59d8da4fbfab7edd14530d40c5",
            "967be5f0f1ab4e488022a983398a1d92",
            "407faeb8c351416db418960bcd5400d3",
            "aa7111b98b7a4d4bb5ff7f8b6a791c4c",
            "36f9f0d0559a47c28d65be6743f74759",
            "167797ead34247ebaeddc982b8eb7f01",
            "91541be8b9fb44a7a61fa5770975ab58",
            "8ac654315f7a486ab28fdd882a3b2fad"
          ]
        },
        "id": "JS6tK2L0gmET",
        "outputId": "a7ef275d-60a1-4528-b52a-d92e268bb2df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: llama-index\n",
            "Version: 0.10.57\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: Jerry Liu\n",
            "Author-email: jerry@llamaindex.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
            "Required-by: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9968bc741a2f46daab66a25ed12eadaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Built hybrid search index with custom BM25 implementation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds reranking to improve retrieval quality.\n",
        "\n",
        "**What is Reranking?**\n",
        "After initial retrieval gets top 10 documents, reranking:\n",
        "1. Takes query + each document\n",
        "2. Scores them more carefully using ColBERT\n",
        "3. Reorders by relevance\n",
        "\n",
        "**Why ColBERT?**\n",
        "- Designed for reranking (not just retrieval)\n",
        "- Considers fine-grained token interactions\n",
        "- Much better at understanding if a document truly answers the query\n",
        "\n",
        "Example: Query \"COVID vaccine side effects in elderly\"\n",
        "- Initial retrieval might get any COVID vaccine papers\n",
        "- Reranking promotes papers specifically about elderly populations"
      ],
      "metadata": {
        "id": "9ppsrWUYgo1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install scikit-learn if not already available\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "except ImportError:\n",
        "    !pip install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Simple reranker class that mimics RAGPretrainedModel interface\n",
        "class SimpleReranker:\n",
        "    \"\"\"Simple TF-IDF based reranker that mimics RAGPretrainedModel interface\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000\n",
        "        )\n",
        "\n",
        "    def rerank(self, query, documents, k=5):\n",
        "        \"\"\"Rerank documents using TF-IDF similarity\"\"\"\n",
        "        if len(documents) == 0:\n",
        "            return []\n",
        "\n",
        "        # Combine query and documents\n",
        "        all_texts = [query] + documents\n",
        "\n",
        "        # Fit and transform\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(all_texts)\n",
        "\n",
        "        # Calculate similarities between query (first item) and documents\n",
        "        query_vec = tfidf_matrix[0:1]\n",
        "        doc_vecs = tfidf_matrix[1:]\n",
        "\n",
        "        similarities = cosine_similarity(query_vec, doc_vecs).flatten()\n",
        "\n",
        "        # Sort by similarity and return top k with indices and scores\n",
        "        indexed_scores = [(i, float(score)) for i, score in enumerate(similarities)]\n",
        "        indexed_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return indexed_scores[:k]\n",
        "\n",
        "# Initialize reranker (keeping same variable name and structure)\n",
        "try:\n",
        "    # Try to use sentence-transformers cross-encoder if available\n",
        "    from sentence_transformers import CrossEncoder\n",
        "\n",
        "    class CrossEncoderReranker:\n",
        "        def __init__(self):\n",
        "            self.model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "        def rerank(self, query, documents, k=5):\n",
        "            if len(documents) == 0:\n",
        "                return []\n",
        "\n",
        "            # Create query-document pairs\n",
        "            pairs = [[query, doc] for doc in documents]\n",
        "\n",
        "            # Get scores\n",
        "            scores = self.model.predict(pairs)\n",
        "\n",
        "            # Sort by score and return top k with indices\n",
        "            indexed_scores = [(i, float(score)) for i, score in enumerate(scores)]\n",
        "            indexed_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return indexed_scores[:k]\n",
        "\n",
        "    reranker = CrossEncoderReranker()\n",
        "    print(\"✓ Cross-encoder reranker initialized\")\n",
        "except:\n",
        "    # Fallback to simple TF-IDF reranker\n",
        "    reranker = SimpleReranker()\n",
        "    print(\"✓ TF-IDF reranker initialized (fallback)\")\n",
        "\n",
        "class RerankedRetriever(BaseRetriever):\n",
        "    def __init__(self, base_retriever, reranker=None, top_k=5):\n",
        "        self.base_retriever = base_retriever\n",
        "        self.reranker = reranker\n",
        "        self.top_k = top_k\n",
        "        super().__init__()\n",
        "\n",
        "    def _retrieve(self, query_bundle):\n",
        "        # Get initial results\n",
        "        initial_results = self.base_retriever.retrieve(query_bundle)\n",
        "\n",
        "        if self.reranker and len(initial_results) > 0:\n",
        "            try:\n",
        "                # Rerank using the available reranker\n",
        "                texts = [node.node.text for node in initial_results]\n",
        "                scores = self.reranker.rerank(\n",
        "                    query=query_bundle.query_str,\n",
        "                    documents=texts,\n",
        "                    k=self.top_k\n",
        "                )\n",
        "\n",
        "                # Return reranked results\n",
        "                reranked = []\n",
        "                for idx, score in scores:\n",
        "                    if idx < len(initial_results):\n",
        "                        result = initial_results[idx]\n",
        "                        # Update score with reranking score\n",
        "                        result.score = score\n",
        "                        reranked.append(result)\n",
        "                return reranked[:self.top_k]\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Reranking failed: {e}, using original order\")\n",
        "                return initial_results[:self.top_k]\n",
        "        else:\n",
        "            # Simple length-based reranking as fallback\n",
        "            return initial_results[:self.top_k]\n",
        "\n",
        "# Create reranked retriever\n",
        "reranked_retriever = RerankedRetriever(\n",
        "    base_retriever=hybrid_retriever,\n",
        "    reranker=reranker,\n",
        "    top_k=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j8niOSxgxQN",
        "outputId": "87674c8d-25fd-4757-f081-35b2a3d51c22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Cross-encoder reranker initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assembles all components into a complete query engine with:\n",
        "\n",
        "1. **Response Synthesis**: How to combine multiple retrieved chunks into one answer\n",
        "   - \"compact\" mode: Fits all context into one LLM call\n",
        "   - \"tree_summarize\": Hierarchical summarization for long contexts\n",
        "\n",
        "2. **Similarity Filtering**: Removes low-relevance chunks (< 0.5 similarity)\n",
        "\n",
        "3. **HyDE (Hypothetical Document Embeddings)**:\n",
        "   - Generates a hypothetical perfect answer\n",
        "   - Searches for documents similar to this ideal answer\n",
        "   - Often improves retrieval for complex queries\n",
        "\n",
        "This is where all components come together into a system that can answer questions.\n"
      ],
      "metadata": {
        "id": "BRDsyV8DgzYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "\n",
        "# Create response synthesizer with citations\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"compact\",  # or \"tree_summarize\" for longer responses\n",
        "    use_async=False,\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "# Add similarity threshold to filter weak matches\n",
        "similarity_processor = SimilarityPostprocessor(\n",
        "    similarity_cutoff=0.5\n",
        ")\n",
        "\n",
        "# Create query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=reranked_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[similarity_processor]\n",
        ")\n",
        "\n",
        "# Add HyDE (Hypothetical Document Embeddings) for better retrieval\n",
        "hyde_transform = HyDEQueryTransform(include_original=True)\n",
        "\n",
        "print(\"✓ Query engine configured with advanced features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-93Te_CUg5Tp",
        "outputId": "dd5ec720-c7c9-4afe-e830-af126ae756ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Query engine configured with advanced features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provides diverse query types to test our system:\n",
        "\n",
        "1. **Factual**: Specific numbers/statistics\n",
        "2. **Comparison**: Contrasting different approaches\n",
        "3. **Analytical**: Understanding challenges/methods\n",
        "4. **Synthesis**: Combining insights across papers\n",
        "\n",
        "These queries test different retrieval challenges:\n",
        "- Factual queries need precise matching\n",
        "- Comparisons need multiple relevant chunks\n",
        "- Synthesis needs good coverage across documents\n",
        "\n",
        "In production, collect real user queries to understand your specific patterns."
      ],
      "metadata": {
        "id": "HtLcVG-jg9F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medical_queries = [\n",
        "    # Factual queries\n",
        "    \"What is the effectiveness of COVID-19 vaccines against the Omicron variant?\",\n",
        "    \"What percentage improvement in medication adherence was seen with AI chatbots for diabetes?\",\n",
        "\n",
        "    # Comparison queries\n",
        "    \"Compare the effectiveness of self-guided vs therapist-supported digital mental health interventions\",\n",
        "    \"How do vaccine effectiveness rates differ between age groups?\",\n",
        "\n",
        "    # Analytical queries\n",
        "    \"What are the main challenges in implementing digital mental health solutions?\",\n",
        "    \"What machine learning approaches are used in diabetes management?\",\n",
        "\n",
        "    # Synthesis queries\n",
        "    \"Summarize the key findings about AI applications in healthcare across all papers\",\n",
        "    \"What are the common limitations mentioned across these studies?\",\n",
        "\n",
        "    # Specific detail queries\n",
        "    \"What was the sample size in the COVID vaccine effectiveness study?\",\n",
        "    \"What is the dropout rate for mental health apps?\"\n",
        "]\n",
        "\n",
        "print(\"✓ Example queries loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdTFbyHqhBy-",
        "outputId": "36b9feec-7b01-4e1b-d3ec-d4eca4212111"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Example queries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executes queries and shows detailed results including:\n",
        "\n",
        "1. **Answer**: The synthesized response\n",
        "2. **Sources**: Which chunks were used\n",
        "3. **Metadata**: Section, paper title, relevance scores\n",
        "\n",
        "This helps you understand:\n",
        "- Is the retrieval getting the right chunks?\n",
        "- Are relevance scores meaningful?\n",
        "- Is the answer properly synthesized?\n",
        "\n",
        "The source attribution is crucial for medical applications where\n",
        "users need to verify claims against original research.\n"
      ],
      "metadata": {
        "id": "gdh9I-QOhFbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🔍 Running example queries...\\n\")\n",
        "\n",
        "for i, query in enumerate(medical_queries[:3], 1):  # Run first 3 queries\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Get response\n",
        "    response = query_engine.query(query)\n",
        "\n",
        "    print(f\"Answer: {response.response}\")\n",
        "    print(f\"\\nSources used: {len(response.source_nodes)}\")\n",
        "\n",
        "    # Show source snippets\n",
        "    for j, source in enumerate(response.source_nodes[:2], 1):\n",
        "        print(f\"\\nSource {j}:\")\n",
        "        print(f\"- Section: {source.node.metadata.get('section', 'Unknown')}\")\n",
        "        print(f\"- Paper: {source.node.metadata.get('title', 'Unknown')}\")\n",
        "        print(f\"- Relevance Score: {source.score:.3f}\")\n",
        "        print(f\"- Text snippet: {source.node.text[:200]}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP2KqV4jhKpI",
        "outputId": "fdcebc71-12d5-492a-a09f-fbaac0b175e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Running example queries...\n",
            "\n",
            "Query 1: What is the effectiveness of COVID-19 vaccines against the Omicron variant?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: The effectiveness of COVID-19 vaccines against the Omicron variant is 64% for infection and 89% for severe disease.\n",
            "\n",
            "Sources used: 2\n",
            "\n",
            "Source 1:\n",
            "- Section: Results\n",
            "- Paper: Real-World Effectiveness of COVID-19 Vaccines: A Multi-Country Analysis\n",
            "- Relevance Score: 6.001\n",
            "- Text snippet: Among fully vaccinated individuals, breakthrough infection rate was 3.2 per 1000 person-months.\n",
            "        Vaccine effectiveness against Delta variant: 88% for infection, 96% for severe disease. Omicron ...\n",
            "\n",
            "Source 2:\n",
            "- Section: Introduction\n",
            "- Paper: Real-World Effectiveness of COVID-19 Vaccines: A Multi-Country Analysis\n",
            "- Relevance Score: 3.600\n",
            "- Text snippet: The rapid development of COVID-19 vaccines necessitated ongoing real-world effectiveness\n",
            "        monitoring. This study leveraged electronic health records from participating healthcare systems to ass...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Query 2: What percentage improvement in medication adherence was seen with AI chatbots for diabetes?\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: 42% increase in medication adherence was seen with AI chatbots for diabetes.\n",
            "\n",
            "Sources used: 1\n",
            "\n",
            "Source 1:\n",
            "- Section: Abstract\n",
            "- Paper: AI-Driven Approaches in Diabetes Management: A Systematic Review\n",
            "- Relevance Score: 6.705\n",
            "- Text snippet: This systematic review examines the application of artificial intelligence in diabetes management,\n",
            "        analyzing 127 studies from 2018-2023. Key findings include improved glycemic control through ...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Query 3: Compare the effectiveness of self-guided vs therapist-supported digital mental health interventions\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: Self-guided interventions showed smaller effects compared to therapist-supported programs.\n",
            "\n",
            "Sources used: 1\n",
            "\n",
            "Source 1:\n",
            "- Section: Abstract\n",
            "- Paper: Digital Mental Health Interventions: Efficacy and Implementation Challenges\n",
            "- Relevance Score: 4.561\n",
            "- Text snippet: Meta-analysis of 89 randomized controlled trials (n=15,492) examining digital mental health\n",
            "        interventions. Cognitive behavioral therapy apps showed moderate effect sizes for depression (d=0.54...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements query routing - sending different query types to specialized engines.\n",
        "\n",
        "**Why Query Routing?**\n",
        "Different queries need different handling:\n",
        "- Statistical queries → Need exact number extraction\n",
        "- Comparisons → Need balanced retrieval from multiple sources  \n",
        "- Synthesis → Need broader coverage\n",
        "\n",
        "The SubQuestionQueryEngine can also:\n",
        "- Break complex queries into simpler sub-questions\n",
        "- Route each sub-question appropriately\n",
        "- Combine results intelligently\n",
        "\n",
        "Example: \"Compare vaccine effectiveness across age groups and identify which group needs boosters most urgently\"\n",
        "→ Sub-question 1: \"What is vaccine effectiveness by age group?\"\n",
        "→ Sub-question 2: \"Which age groups show waning immunity?\"\n"
      ],
      "metadata": {
        "id": "ZvaqJaVnhPAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "# Create specialized query engines for different types of queries\n",
        "statistical_engine = query_engine  # Configure for numerical/statistical queries\n",
        "comparison_engine = query_engine   # Configure for comparative analysis\n",
        "synthesis_engine = query_engine    # Configure for multi-document synthesis\n",
        "\n",
        "# Create query engine tools\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=statistical_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"statistical_search\",\n",
        "            description=\"Best for finding specific numbers, percentages, and statistical results\"\n",
        "        )\n",
        "    ),\n",
        "    QueryEngineTool(\n",
        "        query_engine=comparison_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"comparison_search\",\n",
        "            description=\"Best for comparing different treatments, methods, or outcomes\"\n",
        "        )\n",
        "    ),\n",
        "    QueryEngineTool(\n",
        "        query_engine=synthesis_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"synthesis_search\",\n",
        "            description=\"Best for summarizing findings across multiple studies\"\n",
        "        )\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create sub-question query engine for complex queries\n",
        "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools\n",
        ")\n",
        "\n",
        "print(\"✓ Advanced query routing configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwrlNAVthWtK",
        "outputId": "5e627e4f-dad2-4f53-ee3a-c13441896c42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Advanced query routing configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measures system performance with key metrics:\n",
        "\n",
        "1. **Query Time**: How fast are responses?\n",
        "2. **Source Count**: How many chunks used per query?\n",
        "3. **Relevance Scores**: How confident is the retrieval?\n",
        "4. **Source Diversity**: Are we using multiple papers or over-relying on one?\n",
        "\n",
        "These metrics help identify:\n",
        "- Performance bottlenecks\n",
        "- Retrieval quality issues\n",
        "- Whether you need better chunking/embeddings\n",
        "\n",
        "In production, also track:\n",
        "- User satisfaction (thumbs up/down)\n",
        "- Click-through on sources\n",
        "- Query abandonment rates"
      ],
      "metadata": {
        "id": "79WVh6n9hZw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "def evaluate_retrieval_quality(query_engine, test_queries):\n",
        "    \"\"\"Evaluate retrieval quality metrics\"\"\"\n",
        "    metrics = defaultdict(list)\n",
        "\n",
        "    for query in test_queries:\n",
        "        start_time = time.time()\n",
        "        response = query_engine.query(query)\n",
        "        query_time = time.time() - start_time\n",
        "\n",
        "        # Collect metrics\n",
        "        metrics['query_time'].append(query_time)\n",
        "        metrics['num_sources'].append(len(response.source_nodes))\n",
        "        metrics['avg_relevance_score'].append(\n",
        "            np.mean([node.score for node in response.source_nodes])\n",
        "        )\n",
        "\n",
        "        # Check source diversity (different papers)\n",
        "        unique_papers = set(\n",
        "            node.node.metadata.get('title', 'Unknown')\n",
        "            for node in response.source_nodes\n",
        "        )\n",
        "        metrics['source_diversity'].append(len(unique_papers))\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary = {}\n",
        "    for metric, values in metrics.items():\n",
        "        summary[metric] = {\n",
        "            'mean': np.mean(values),\n",
        "            'std': np.std(values),\n",
        "            'min': np.min(values),\n",
        "            'max': np.max(values)\n",
        "        }\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Run evaluation\n",
        "print(\"📊 Evaluating retrieval quality...\")\n",
        "eval_results = evaluate_retrieval_quality(query_engine, medical_queries[:5])\n",
        "\n",
        "for metric, stats in eval_results.items():\n",
        "    print(f\"\\n{metric}:\")\n",
        "    print(f\"  Mean: {stats['mean']:.3f}\")\n",
        "    print(f\"  Std:  {stats['std']:.3f}\")\n",
        "    print(f\"  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzuVSIjuhfZB",
        "outputId": "1e98c3f7-9045-4cd6-bcaf-4787a03ac169"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Evaluating retrieval quality...\n",
            "\n",
            "query_time:\n",
            "  Mean: 5.226\n",
            "  Std:  1.038\n",
            "  Range: [3.973, 6.535]\n",
            "\n",
            "num_sources:\n",
            "  Mean: 1.400\n",
            "  Std:  0.490\n",
            "  Range: [1.000, 2.000]\n",
            "\n",
            "avg_relevance_score:\n",
            "  Mean: 4.599\n",
            "  Std:  1.287\n",
            "  Range: [2.683, 6.705]\n",
            "\n",
            "source_diversity:\n",
            "  Mean: 1.000\n",
            "  Std:  0.000\n",
            "  Range: [1.000, 1.000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for the UI\n",
        "!pip install gradio plotly pandas\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# Enhanced RAG Analysis Functions\n",
        "def analyze_retrieval_components(query, top_k=10):\n",
        "    \"\"\"Analyze different retrieval components separately\"\"\"\n",
        "\n",
        "    # Import QueryBundle for proper query handling\n",
        "    from llama_index.core.schema import QueryBundle\n",
        "\n",
        "    # Create proper QueryBundle object\n",
        "    query_bundle = QueryBundle(query_str=query)\n",
        "\n",
        "    # Get results from different retrievers\n",
        "    vector_results = vector_retriever.retrieve(query_bundle)\n",
        "    bm25_results = bm25_retriever.retrieve(query_bundle)\n",
        "    hybrid_results = hybrid_retriever.retrieve(query_bundle)\n",
        "\n",
        "    # Format results for comparison\n",
        "    def format_results(results, retriever_type):\n",
        "        formatted = []\n",
        "        for i, node in enumerate(results[:top_k]):\n",
        "            formatted.append({\n",
        "                'rank': i + 1,\n",
        "                'retriever': retriever_type,\n",
        "                'score': round(node.score, 4),\n",
        "                'section': node.node.metadata.get('section', 'Unknown'),\n",
        "                'paper': node.node.metadata.get('title', 'Unknown')[:50] + '...',\n",
        "                'text_preview': node.node.text[:150] + '...',\n",
        "                'node_id': node.node.node_id[:8]  # Short ID for tracking\n",
        "            })\n",
        "        return formatted\n",
        "\n",
        "    vector_data = format_results(vector_results, 'Vector (Semantic)')\n",
        "    bm25_data = format_results(bm25_results, 'BM25 (Keyword)')\n",
        "    hybrid_data = format_results(hybrid_results, 'Hybrid')\n",
        "\n",
        "    return vector_data, bm25_data, hybrid_data\n",
        "\n",
        "def create_retrieval_comparison_plot(vector_data, bm25_data, hybrid_data):\n",
        "    \"\"\"Create visualization comparing different retrieval methods\"\"\"\n",
        "\n",
        "    # Combine data for plotting\n",
        "    all_data = vector_data + bm25_data + hybrid_data\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    if df.empty:\n",
        "        return go.Figure().add_annotation(text=\"No data to display\",\n",
        "                                        xref=\"paper\", yref=\"paper\",\n",
        "                                        x=0.5, y=0.5, showarrow=False)\n",
        "\n",
        "    # Create subplots with pie chart support\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Retrieval Scores by Method', 'Score Distribution',\n",
        "                       'Section Coverage', 'Paper Diversity'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"type\": \"domain\"}]]  # domain type for pie chart\n",
        "    )\n",
        "\n",
        "    # Plot 1: Scores by rank and method\n",
        "    colors = {'Vector (Semantic)': '#1f77b4', 'BM25 (Keyword)': '#ff7f0e', 'Hybrid': '#2ca02c'}\n",
        "    for method in df['retriever'].unique():\n",
        "        method_data = df[df['retriever'] == method]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=method_data['rank'], y=method_data['score'],\n",
        "                      mode='lines+markers', name=method,\n",
        "                      line=dict(color=colors.get(method, '#333333')),\n",
        "                      hovertemplate='<b>%{fullData.name}</b><br>' +\n",
        "                                  'Rank: %{x}<br>' +\n",
        "                                  'Score: %{y}<br>' +\n",
        "                                  '<extra></extra>'),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "    # Plot 2: Score distribution\n",
        "    for method in df['retriever'].unique():\n",
        "        method_scores = df[df['retriever'] == method]['score']\n",
        "        fig.add_trace(\n",
        "            go.Box(y=method_scores, name=method,\n",
        "                  marker_color=colors.get(method, '#333333'),\n",
        "                  showlegend=False),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "    # Plot 3: Section coverage\n",
        "    section_counts = df.groupby(['retriever', 'section']).size().reset_index(name='count')\n",
        "    for method in section_counts['retriever'].unique():\n",
        "        method_sections = section_counts[section_counts['retriever'] == method]\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=method_sections['section'], y=method_sections['count'],\n",
        "                  name=method, marker_color=colors.get(method, '#333333'),\n",
        "                  showlegend=False),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "    # Plot 4: Paper diversity (pie chart for hybrid method)\n",
        "    hybrid_papers = df[df['retriever'] == 'Hybrid']['paper'].value_counts()\n",
        "    if not hybrid_papers.empty:\n",
        "        fig.add_trace(\n",
        "            go.Pie(labels=hybrid_papers.index, values=hybrid_papers.values,\n",
        "                  name=\"Paper Distribution\", showlegend=False),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "    fig.update_layout(height=800, title_text=\"RAG Retrieval Analysis Dashboard\")\n",
        "    fig.update_xaxes(title_text=\"Rank\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Relevance Score\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Score\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Section\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def create_query_performance_plot(metrics_history):\n",
        "    \"\"\"Create performance metrics visualization\"\"\"\n",
        "    if not metrics_history:\n",
        "        return go.Figure().add_annotation(text=\"No metrics data available\",\n",
        "                                        xref=\"paper\", yref=\"paper\",\n",
        "                                        x=0.5, y=0.5, showarrow=False)\n",
        "\n",
        "    df = pd.DataFrame(metrics_history)\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Query Response Time', 'Number of Sources Retrieved',\n",
        "                       'Average Relevance Score', 'Source Diversity'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "    )\n",
        "\n",
        "    # Response time trend\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(len(df))), y=df['query_time'],\n",
        "                  mode='lines+markers', name='Response Time',\n",
        "                  line=dict(color='#1f77b4')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Number of sources\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=list(range(len(df))), y=df['num_sources'],\n",
        "              name='Sources', marker_color='#ff7f0e'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Relevance scores\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(len(df))), y=df['avg_relevance_score'],\n",
        "                  mode='lines+markers', name='Avg Relevance',\n",
        "                  line=dict(color='#2ca02c')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Source diversity\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=list(range(len(df))), y=df['source_diversity'],\n",
        "              name='Diversity', marker_color='#d62728'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(height=600, title_text=\"Query Performance Metrics\", showlegend=False)\n",
        "    fig.update_xaxes(title_text=\"Query Number\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Query Number\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Query Number\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Query Number\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"Unique Papers\", row=2, col=2)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Global variables to track metrics\n",
        "metrics_history = []\n",
        "\n",
        "def query_rag_system(query, retrieval_method=\"Hybrid\", top_k=5, similarity_threshold=0.3):\n",
        "    \"\"\"Main function to query the RAG system with visualization\"\"\"\n",
        "    global metrics_history\n",
        "\n",
        "    if not query.strip():\n",
        "        return \"Please enter a query.\", None, None, \"No metrics available.\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Create temporary reranked retriever for the selected method\n",
        "        if retrieval_method == \"Hybrid\":\n",
        "            selected_retriever = RerankedRetriever(\n",
        "                base_retriever=hybrid_retriever,\n",
        "                reranker=reranker,\n",
        "                top_k=top_k\n",
        "            )\n",
        "        else:\n",
        "            # For non-hybrid methods, use the base retriever directly\n",
        "            if retrieval_method == \"Vector (Semantic)\":\n",
        "                base_retriever = vector_retriever\n",
        "            else:  # BM25 (Keyword)\n",
        "                base_retriever = bm25_retriever\n",
        "\n",
        "            selected_retriever = RerankedRetriever(\n",
        "                base_retriever=base_retriever,\n",
        "                reranker=None,  # No reranking for single methods\n",
        "                top_k=top_k\n",
        "            )\n",
        "\n",
        "        # Create temporary query engine with selected retriever\n",
        "        from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "        from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "        from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "        temp_query_engine = RetrieverQueryEngine(\n",
        "            retriever=selected_retriever,\n",
        "            response_synthesizer=get_response_synthesizer(response_mode=\"compact\"),\n",
        "            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=similarity_threshold)]\n",
        "        )\n",
        "\n",
        "        # Get response\n",
        "        response = temp_query_engine.query(query)\n",
        "        query_time = time.time() - start_time\n",
        "\n",
        "        # Format response with sources\n",
        "        formatted_response = f\"**Answer:** {response.response}\\n\\n\"\n",
        "        formatted_response += f\"**Sources Used:** {len(response.source_nodes)}\\n\\n\"\n",
        "\n",
        "        if response.source_nodes:\n",
        "            formatted_response += \"**Source Details:**\\n\"\n",
        "            for i, source in enumerate(response.source_nodes, 1):\n",
        "                formatted_response += f\"\\n**Source {i}:**\\n\"\n",
        "                formatted_response += f\"- **Section:** {source.node.metadata.get('section', 'Unknown')}\\n\"\n",
        "                formatted_response += f\"- **Paper:** {source.node.metadata.get('title', 'Unknown')}\\n\"\n",
        "                formatted_response += f\"- **Relevance Score:** {source.score:.3f}\\n\"\n",
        "                formatted_response += f\"- **Text:** {source.node.text[:200]}...\\n\"\n",
        "\n",
        "        # Collect metrics\n",
        "        unique_papers = set(node.node.metadata.get('title', 'Unknown') for node in response.source_nodes)\n",
        "        metrics = {\n",
        "            'query_time': query_time,\n",
        "            'num_sources': len(response.source_nodes),\n",
        "            'avg_relevance_score': np.mean([node.score for node in response.source_nodes]) if response.source_nodes else 0,\n",
        "            'source_diversity': len(unique_papers),\n",
        "            'query': query[:30] + '...' if len(query) > 30 else query\n",
        "        }\n",
        "        metrics_history.append(metrics)\n",
        "\n",
        "        # Generate visualizations\n",
        "        vector_data, bm25_data, hybrid_data = analyze_retrieval_components(query, top_k)\n",
        "        comparison_plot = create_retrieval_comparison_plot(vector_data, bm25_data, hybrid_data)\n",
        "        performance_plot = create_query_performance_plot(metrics_history)\n",
        "\n",
        "        # Format metrics summary\n",
        "        metrics_summary = f\"\"\"\n",
        "**Query Metrics:**\n",
        "- Response Time: {query_time:.3f} seconds\n",
        "- Sources Retrieved: {len(response.source_nodes)}\n",
        "- Average Relevance: {metrics['avg_relevance_score']:.3f}\n",
        "- Unique Papers: {metrics['source_diversity']}\n",
        "- Retrieval Method: {retrieval_method}\n",
        "        \"\"\"\n",
        "\n",
        "        return formatted_response, comparison_plot, performance_plot, metrics_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", None, None, f\"Error occurred: {str(e)}\"\n",
        "\n",
        "def reset_metrics():\n",
        "    \"\"\"Reset metrics history\"\"\"\n",
        "    global metrics_history\n",
        "    metrics_history = []\n",
        "    return \"Metrics history cleared!\", None\n",
        "\n",
        "# Create Gradio Interface\n",
        "def create_rag_interface():\n",
        "    with gr.Blocks(title=\"Advanced RAG Medical Literature System\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 🏥 Advanced RAG Medical Literature System\n",
        "\n",
        "        This interactive interface allows you to query medical literature using different retrieval methods\n",
        "        and visualize how the system performs. You can compare semantic search, keyword search, and hybrid approaches.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"## 🔍 Query Interface\")\n",
        "\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Medical Query\",\n",
        "                    placeholder=\"Enter your medical question (e.g., 'What is the effectiveness of COVID vaccines?')\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    retrieval_method = gr.Dropdown(\n",
        "                        choices=[\"Hybrid\", \"Vector (Semantic)\", \"BM25 (Keyword)\"],\n",
        "                        value=\"Hybrid\",\n",
        "                        label=\"Retrieval Method\"\n",
        "                    )\n",
        "\n",
        "                    top_k = gr.Slider(\n",
        "                        minimum=1, maximum=20, value=5, step=1,\n",
        "                        label=\"Max Sources to Retrieve\"\n",
        "                    )\n",
        "\n",
        "                similarity_threshold = gr.Slider(\n",
        "                    minimum=0.0, maximum=1.0, value=0.3, step=0.1,\n",
        "                    label=\"Similarity Threshold\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    submit_btn = gr.Button(\"🚀 Query System\", variant=\"primary\")\n",
        "                    reset_btn = gr.Button(\"🔄 Reset Metrics\", variant=\"secondary\")\n",
        "\n",
        "                # Example queries\n",
        "                gr.Markdown(\"### 📝 Example Queries:\")\n",
        "                example_queries = [\n",
        "                    \"What is the effectiveness of COVID-19 vaccines against Omicron?\",\n",
        "                    \"How do AI chatbots improve diabetes medication adherence?\",\n",
        "                    \"What are challenges in digital mental health interventions?\",\n",
        "                    \"Compare self-guided vs therapist-supported mental health apps\",\n",
        "                    \"What machine learning approaches are used in diabetes management?\"\n",
        "                ]\n",
        "\n",
        "                for example in example_queries:\n",
        "                    gr.Button(example, size=\"sm\").click(\n",
        "                        lambda x=example: x, outputs=query_input\n",
        "                    )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"## 📊 Results & Analysis\")\n",
        "\n",
        "                response_output = gr.Markdown(label=\"System Response\")\n",
        "\n",
        "                with gr.Tabs():\n",
        "                    with gr.TabItem(\"🔄 Retrieval Comparison\"):\n",
        "                        comparison_plot = gr.Plot(label=\"Retrieval Methods Comparison\")\n",
        "\n",
        "                    with gr.TabItem(\"📈 Performance Metrics\"):\n",
        "                        performance_plot = gr.Plot(label=\"Query Performance Over Time\")\n",
        "\n",
        "                    with gr.TabItem(\"📋 Metrics Summary\"):\n",
        "                        metrics_output = gr.Markdown(label=\"Current Query Metrics\")\n",
        "\n",
        "        # Event handlers\n",
        "        submit_btn.click(\n",
        "            fn=query_rag_system,\n",
        "            inputs=[query_input, retrieval_method, top_k, similarity_threshold],\n",
        "            outputs=[response_output, comparison_plot, performance_plot, metrics_output]\n",
        "        )\n",
        "\n",
        "        reset_btn.click(\n",
        "            fn=reset_metrics,\n",
        "            outputs=[metrics_output, performance_plot]\n",
        "        )\n",
        "\n",
        "        # Additional information\n",
        "        with gr.Accordion(\"ℹ️ System Information\", open=False):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### How It Works:\n",
        "\n",
        "            - **Vector (Semantic)**: Uses BioBERT embeddings to find semantically similar content\n",
        "            - **BM25 (Keyword)**: Traditional keyword-based search using term frequency\n",
        "            - **Hybrid**: Combines both approaches with weighted scoring\n",
        "\n",
        "            ### Visualizations:\n",
        "\n",
        "            - **Retrieval Comparison**: Shows how different methods rank the same documents\n",
        "            - **Performance Metrics**: Tracks response time, source count, and relevance over multiple queries\n",
        "            - **Section Coverage**: Displays which paper sections are being retrieved\n",
        "            - **Paper Diversity**: Shows distribution across different research papers\n",
        "\n",
        "            ### Tips:\n",
        "\n",
        "            - Try the same query with different retrieval methods to see variations\n",
        "            - Use specific medical terms for better BM25 results\n",
        "            - Use conceptual questions for better semantic search results\n",
        "            - Adjust similarity threshold to filter out less relevant results\n",
        "            \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "demo = create_rag_interface()\n",
        "demo.launch(\n",
        "    share=True,  # Creates public link for sharing\n",
        "    server_name=\"0.0.0.0\",  # Makes it accessible in Colab\n",
        "    server_port=7860,\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l_kKnPUtvH42",
        "outputId": "75e454c1-1378-4e65-cf93-d9fb568daa39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e368302134c3846610.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e368302134c3846610.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiIaWBFcyZ-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}