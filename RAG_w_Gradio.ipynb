{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlBss5PtlyHF+hgD3mkGxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiguru-pro/k-ai-engineers/blob/main/RAG_w_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† RAG Workshop with Gradio Interface - Visual Educational Demo\n",
        "# Kognitic AI for Engineers - Progressive Examples\n",
        "\n",
        "\n",
        "ENHANCED WORKSHOP WITH GRADIO UI:\n",
        "1. üìö BASIC RAG: Simple document Q&A with clinical trial protocols\n",
        "2. üîç INTERMEDIATE RAG: Multi-document analysis with reranking\n",
        "3. üé® GRADIO UI: Beautiful visual interface for better learning experience\n",
        "\n",
        "All original functionality preserved - just adding visual interface!\n",
        "\n"
      ],
      "metadata": {
        "id": "cEj6qiV6QAWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for Google Colab\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Install packages if not already installed\n",
        "try:\n",
        "    import sentence_transformers\n",
        "    import faiss\n",
        "    import openai\n",
        "    import gradio as gr\n",
        "except ImportError:\n",
        "    print(\"Installing required packages...\")\n",
        "    os.system('pip install openai sentence-transformers faiss-cpu rank-bm25 numpy pandas matplotlib seaborn scikit-learn gradio -q')\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "import gradio as gr\n",
        "\n",
        "print(\"üîß EMBEDDING & VECTOR DB SETUP:\")\n",
        "print(f\"‚úÖ SentenceTransformers: Available\")\n",
        "print(f\"‚úÖ FAISS: Available\")\n",
        "print(f\"‚úÖ OpenAI: Available\")\n",
        "print(f\"‚úÖ Gradio: Available\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjXtQ1afQLkh",
        "outputId": "20c7f234-ed66-4f08-b249-db3da594534a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "üîß EMBEDDING & VECTOR DB SETUP:\n",
            "‚úÖ SentenceTransformers: Available\n",
            "‚úÖ FAISS: Available\n",
            "‚úÖ OpenAI: Available\n",
            "‚úÖ Gradio: Available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicRAG:\n",
        "    \"\"\"\n",
        "    Simplest RAG implementation for clinical trial protocol analysis.\n",
        "    Shows core concepts without complexity.\n",
        "\n",
        "    üéØ EDUCATIONAL FOCUS:\n",
        "    - Basic RAG pipeline: Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate\n",
        "    - Using HuggingFace embedding models\n",
        "    - Simple vector similarity search with FAISS\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "        print(f\"ü§ñ Loading embedding model: {embedding_model}\")\n",
        "        self.encoder = SentenceTransformer(embedding_model)  # HuggingFace model\n",
        "\n",
        "        self.documents = []\n",
        "        self.embeddings = []\n",
        "        self.index = None\n",
        "\n",
        "        print(f\"‚úÖ Model loaded: {self.encoder.get_sentence_embedding_dimension()}-dimensional embeddings\")\n",
        "\n",
        "    def add_document(self, text: str, metadata: Dict = None):\n",
        "        \"\"\"Add a document to the knowledge base with chunking demonstration\"\"\"\n",
        "\n",
        "        print(f\"üìÑ Processing document ({len(text)} characters)...\")\n",
        "\n",
        "        # Demonstrate chunking impact\n",
        "        chunks = self._smart_chunk_document(text)\n",
        "        print(f\"üìö Created {len(chunks)} chunks using sentence-based chunking\")\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_metadata = {**(metadata or {}), 'chunk_id': i, 'chunk_text_preview': chunk[:50]}\n",
        "\n",
        "            self.documents.append({\n",
        "                'text': chunk,\n",
        "                'metadata': chunk_metadata\n",
        "            })\n",
        "\n",
        "            # Create embedding using HuggingFace model\n",
        "            print(f\"üîÑ Creating embedding for chunk {i+1}...\")\n",
        "            embedding = self.encoder.encode(chunk)\n",
        "            self.embeddings.append(embedding)\n",
        "\n",
        "        # Rebuild FAISS index\n",
        "        self._build_faiss_index()\n",
        "        print(f\"üóÑÔ∏è FAISS index updated with {len(self.documents)} total chunks\")\n",
        "\n",
        "    def _smart_chunk_document(self, text: str, sentences_per_chunk: int = 3) -> List[str]:\n",
        "        \"\"\"Demonstrate sentence-based chunking (better than naive character splitting)\"\"\"\n",
        "\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "        chunks = []\n",
        "        for i in range(0, len(sentences), sentences_per_chunk):\n",
        "            chunk = '. '.join(sentences[i:i+sentences_per_chunk])\n",
        "            if chunk:\n",
        "                chunks.append(chunk + '.')\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _build_faiss_index(self):\n",
        "        \"\"\"Build FAISS index for fast similarity search\"\"\"\n",
        "        if self.embeddings:\n",
        "            embeddings_array = np.array(self.embeddings).astype('float32')\n",
        "\n",
        "            # Using Inner Product (IP) for cosine similarity\n",
        "            self.index = faiss.IndexFlatIP(embeddings_array.shape[1])\n",
        "\n",
        "            # Normalize embeddings for cosine similarity\n",
        "            faiss.normalize_L2(embeddings_array)\n",
        "            self.index.add(embeddings_array)\n",
        "\n",
        "            print(f\"üóÑÔ∏è FAISS index built: {self.index.ntotal} vectors, {self.index.d} dimensions\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Retrieve most relevant documents using vector similarity\"\"\"\n",
        "        if not self.index:\n",
        "            return []\n",
        "\n",
        "        print(f\"üîç Searching for: '{query}'\")\n",
        "\n",
        "        # Encode query using same model\n",
        "        query_embedding = self.encoder.encode(query).astype('float32').reshape(1, -1)\n",
        "        faiss.normalize_L2(query_embedding)  # Normalize for cosine similarity\n",
        "\n",
        "        # Search using FAISS\n",
        "        scores, indices = self.index.search(query_embedding, min(top_k, len(self.documents)))\n",
        "\n",
        "        print(f\"üìä Retrieved {len(indices[0])} results\")\n",
        "\n",
        "        # Return results with similarity scores\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append({\n",
        "                    'document': self.documents[idx],\n",
        "                    'similarity_score': float(score),\n",
        "                    'embedding_model': self.encoder._modules['0'].auto_model.name_or_path\n",
        "                })\n",
        "                print(f\"   üìÑ Chunk {idx}: similarity={score:.3f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:\n",
        "        \"\"\"Generate answer using retrieved context\"\"\"\n",
        "\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Document {i+1}: {doc['document']['text']}\"\n",
        "            for i, doc in enumerate(context_docs)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a clinical research specialist analyzing trial protocols.\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Answer based ONLY on the provided context\n",
        "- If information isn't in the context, say \"Based on the provided documents, I cannot find...\"\n",
        "- Be specific and cite which document your answer comes from\n",
        "- Focus on clinical accuracy and regulatory compliance\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=400,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = self.retrieve(question, top_k=3)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            return {\n",
        "                'answer': \"No relevant documents found in the knowledge base.\",\n",
        "                'sources': [],\n",
        "                'method': 'basic_rag'\n",
        "            }\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.generate_answer(question, retrieved_docs)\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': [doc['document']['metadata'] for doc in retrieved_docs],\n",
        "            'scores': [doc['similarity_score'] for doc in retrieved_docs],\n",
        "            'method': 'basic_rag'\n",
        "        }\n"
      ],
      "metadata": {
        "id": "XfELgAT3Qbzp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate RAG"
      ],
      "metadata": {
        "id": "jntgpq1jQhuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Document:\n",
        "    \"\"\"Enhanced document structure with metadata\"\"\"\n",
        "    id: str\n",
        "    text: str\n",
        "    metadata: Dict[str, Any]\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "\n",
        "class IntermediateRAG:\n",
        "    \"\"\"\n",
        "    More sophisticated RAG with PDF processing, reranking and multi-document synthesis.\n",
        "    Demonstrates cross-document querying across multiple clinical trial protocols.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.documents: List[Document] = []\n",
        "        self.index = None\n",
        "\n",
        "        # BM25 for keyword-based retrieval\n",
        "        self.bm25 = None\n",
        "        self.tokenized_docs = []\n",
        "\n",
        "    def create_sample_pdfs(self) -> List[str]:\n",
        "        \"\"\"Create sample PDF content files for Google Colab compatibility\"\"\"\n",
        "\n",
        "        # Sample PDF contents (realistic clinical trial data)\n",
        "        pdf_contents = {\n",
        "            \"KEYNOTE-189_Protocol.txt\": \"KEYNOTE-189 Clinical Trial Protocol\\nMK-3475-189: Phase III Pembrolizumab + Pemetrexed + Platinum Study\\n\\nSTUDY SUMMARY:\\nThis randomized, double-blind, placebo-controlled Phase III study compares pembrolizumab in combination with pemetrexed and carboplatin or cisplatin versus placebo in combination with pemetrexed and carboplatin or cisplatin in participants with previously untreated metastatic nonsquamous NSCLC.\\n\\nPRIMARY OBJECTIVES:\\n- Compare overall survival (OS) of pembrolizumab combination vs placebo combination\\n- Compare progression-free survival (PFS) of pembrolizumab combination vs placebo combination\\n\\nSTUDY DESIGN:\\n- Phase III, randomized, double-blind, placebo-controlled\\n- Population: Metastatic nonsquamous NSCLC, first-line treatment\\n- Sample Size: Approximately 600 participants\\n- Randomization: 2:1 (pembrolizumab:placebo)\\n\\nKEY STUDY RESULTS:\\nPrimary Endpoint Results:\\n- Overall Survival: 22.0 vs 10.7 months (HR 0.56, 95% CI 0.45-0.70, p<0.001)\\n- Progression-Free Survival: 9.0 vs 4.9 months (HR 0.48, 95% CI 0.40-0.58, p<0.001)\\n\\nSecondary Endpoints:\\n- Objective Response Rate: 48% vs 19% (p<0.001)\\n- Duration of Response: 11.2 vs 7.8 months\\n- Disease Control Rate: 71% vs 52%\\n\\nSafety Profile:\\n- Grade 3+ Treatment-Related AEs: 71% vs 66%\\n- Most common AEs: Fatigue (56%), nausea (45%), anemia (34%)\\n- Immune-related AEs: Hypothyroidism (9%), pneumonitis (5%), hepatitis (2%)\\n- Treatment discontinuation due to AEs: 13% vs 8%\",\n",
        "\n",
        "            \"CheckMate-227_Protocol.txt\": \"CheckMate-227 Clinical Trial Protocol\\nCA209-227: Phase III Nivolumab + Ipilimumab vs Chemotherapy Study\\n\\nSTUDY OVERVIEW:\\nThis randomized, open-label, multi-center Phase III study evaluates nivolumab combined with ipilimumab versus platinum-based doublet chemotherapy in first-line treatment of stage IV or recurrent non-small cell lung cancer.\\n\\nPRIMARY OBJECTIVES:\\n- Evaluate OS of nivolumab + ipilimumab vs chemotherapy in TMB ‚â•10 mut/Mb patients\\n- Evaluate OS of nivolumab + ipilimumab vs chemotherapy in PD-L1 ‚â•1% patients\\n\\nSTUDY DESIGN:\\n- Phase III, randomized, open-label, multi-center\\n- Population: Previously untreated advanced/metastatic NSCLC\\n- Total Enrollment: 1,739 participants\\n- Stratification: Histology, sex, geographic region\\n\\nKEY STUDY RESULTS:\\nPrimary Endpoint Results (TMB ‚â•10 mut/Mb):\\n- Overall Survival: 17.3 vs 14.9 months (HR 0.79, 95% CI 0.65-0.96, p=0.0154)\\n- Progression-Free Survival: 7.2 vs 5.5 months (HR 0.82, 95% CI 0.69-0.97)\\n- Objective Response Rate: 36% vs 30%\\n\\nSafety Profile:\\n- Grade 3-4 Treatment-Related AEs: 31% vs 36%\\n- Most common irAEs: Skin reactions (34%), diarrhea (17%), hepatitis (8%)\\n- Pneumonitis: 7% (Grade 3-4: 2%)\\n- Treatment discontinuation due to AEs: 18%\\n- Corticosteroid use for irAE management: 35%\",\n",
        "\n",
        "            \"BEACON-CRC_Protocol.txt\": \"BEACON CRC Clinical Trial Protocol\\nBRF117019: Phase III Encorafenib + Binimetinib + Cetuximab Study\\n\\nSTUDY RATIONALE:\\nBackground:\\nBRAF V600E mutations occur in approximately 8-12% of metastatic colorectal cancer (mCRC) patients and are associated with poor prognosis, resistance to anti-EGFR therapy when used as monotherapy, and limited treatment options in later-line settings.\\n\\nSTUDY OBJECTIVES:\\nPrimary Objective:\\nDemonstrate superior overall survival (OS) of encorafenib + cetuximab versus standard of care in patients with BRAF V600E-mutant mCRC who have received 1-2 prior systemic therapies.\\n\\nSTUDY DESIGN:\\nOverall Design:\\n- Phase III, randomized, open-label, active-controlled\\n- Population: BRAF V600E-mutant mCRC, 1-2 prior therapies\\n- Randomization: 1:1:1 across three treatment arms\\n- Stratification: ECOG PS (0 vs 1), prior bevacizumab, region\\n\\nKEY STUDY RESULTS:\\nPrimary Endpoint Results:\\n- Overall Survival (Doublet vs Control): 9.0 vs 5.4 months (HR 0.52, p<0.001)\\n- Overall Survival (Triplet vs Control): 9.3 vs 5.4 months (HR 0.54, p<0.001)\\n\\nSecondary Endpoints:\\n- Progression-Free Survival: 4.5 vs 1.5 months (HR 0.42)\\n- Objective Response Rate: 26% vs 2% (doublet), 27% vs 2% (triplet)\\n- Disease Control Rate: 61% vs 22% (doublet), 65% vs 22% (triplet)\\n\\nClinical Significance:\\n- First targeted therapy success in BRAF-mutant mCRC\\n- Represents major breakthrough for historically poor prognosis population\\n- Demonstrates importance of combination approach to overcome resistance\"\n",
        "        }\n",
        "\n",
        "        print(\"üìÑ Creating sample clinical trial protocol files for Google Colab...\")\n",
        "\n",
        "        # Create text files (Colab-friendly alternative to PDFs)\n",
        "        created_files = []\n",
        "        for filename, content in pdf_contents.items():\n",
        "            try:\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "                created_files.append(filename)\n",
        "                print(f\"   ‚úÖ Created {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error creating {filename}: {e}\")\n",
        "\n",
        "        print(f\"‚úÖ Created {len(created_files)} protocol files ready for RAG processing\")\n",
        "        return created_files\n",
        "\n",
        "    def load_pdf_content(self, file_path: str) -> str:\n",
        "        \"\"\"Load content from protocol file (Colab-friendly)\"\"\"\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "            print(f\"üìñ Loaded {len(content)} characters from {file_path}\")\n",
        "            return content\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ö†Ô∏è  File not found: {file_path}\")\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def chunk_document(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:\n",
        "        \"\"\"Advanced chunking with overlap for better retrieval\"\"\"\n",
        "\n",
        "        print(f\"üî™ Chunking document: {len(text)} chars ‚Üí chunks of ~{chunk_size} chars with {overlap} overlap\")\n",
        "\n",
        "        # Split by sections first (assuming sections are separated by multiple newlines)\n",
        "        sections = text.split('\\n\\n')\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for section in sections:\n",
        "            # If adding this section would exceed chunk size, finalize current chunk\n",
        "            if len(current_chunk) + len(section) > chunk_size and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Start new chunk with overlap from previous chunk\n",
        "                words = current_chunk.split()\n",
        "                overlap_text = ' '.join(words[-overlap//10:]) if len(words) > overlap//10 else \"\"\n",
        "                current_chunk = overlap_text + '\\n\\n' + section\n",
        "            else:\n",
        "                current_chunk += '\\n\\n' + section if current_chunk else section\n",
        "\n",
        "        # Add the last chunk\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        print(f\"üìö Created {len(chunks)} chunks (avg length: {np.mean([len(c) for c in chunks]):.0f} chars)\")\n",
        "        return chunks\n",
        "\n",
        "    def load_pdf_documents(self, pdf_files: List[str]):\n",
        "        \"\"\"Load and process multiple protocol documents\"\"\"\n",
        "\n",
        "        print(f\"\\nüìö LOADING {len(pdf_files)} CLINICAL TRIAL DOCUMENTS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        total_chunks = 0\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            print(f\"\\nüìÑ Processing: {pdf_file}\")\n",
        "\n",
        "            # Extract basic metadata from filename\n",
        "            study_name = pdf_file.replace('_Protocol.txt', '').replace('.txt', '')\n",
        "\n",
        "            # Load content\n",
        "            content = self.load_pdf_content(pdf_file)\n",
        "            if not content:\n",
        "                continue\n",
        "\n",
        "            # Extract enhanced metadata from content\n",
        "            metadata = self.extract_metadata_from_content(content, study_name)\n",
        "            print(f\"üè∑Ô∏è  Extracted metadata: {metadata}\")\n",
        "\n",
        "            # Chunk the document with overlap\n",
        "            chunks = self.chunk_document(content)\n",
        "\n",
        "            print(f\"üìë {study_name}: {len(chunks)} chunks created\")\n",
        "\n",
        "            # Add each chunk as a separate document with rich metadata\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_id = f\"{study_name}_chunk_{i+1}\"\n",
        "                chunk_metadata = {\n",
        "                    **metadata,\n",
        "                    'chunk_id': chunk_id,\n",
        "                    'chunk_number': i+1,\n",
        "                    'total_chunks': len(chunks),\n",
        "                    'source_file': pdf_file,\n",
        "                    'chunk_size': len(chunk),\n",
        "                    'has_results': 'results' in chunk.lower(),\n",
        "                    'has_safety': any(term in chunk.lower() for term in ['adverse', 'safety', 'toxicity']),\n",
        "                    'has_efficacy': any(term in chunk.lower() for term in ['survival', 'response', 'efficacy'])\n",
        "                }\n",
        "\n",
        "                self.add_document(chunk_id, chunk, chunk_metadata)\n",
        "                total_chunks += 1\n",
        "\n",
        "        print(f\"\\n‚úÖ DOCUMENT LOADING COMPLETE\")\n",
        "        print(f\"üìä Total chunks indexed: {total_chunks}\")\n",
        "        print(f\"ü§ñ Embedding model: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        print(f\"üóÑÔ∏è Vector database: FAISS (IndexFlatIP)\")\n",
        "\n",
        "    def extract_metadata_from_content(self, content: str, study_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract metadata from document content using pattern matching\"\"\"\n",
        "\n",
        "        metadata = {\n",
        "            'study': study_name,\n",
        "            'document_type': 'clinical_protocol'\n",
        "        }\n",
        "\n",
        "        # Extract key information using simple regex patterns\n",
        "\n",
        "        # Extract indication\n",
        "        if 'NSCLC' in content or 'lung cancer' in content.lower():\n",
        "            metadata['indication'] = 'NSCLC'\n",
        "        elif 'colorectal' in content.lower() or 'CRC' in content:\n",
        "            metadata['indication'] = 'Colorectal Cancer'\n",
        "\n",
        "        # Extract line of therapy\n",
        "        if 'first-line' in content.lower():\n",
        "            metadata['line_of_therapy'] = 'first-line'\n",
        "        elif 'second-line' in content.lower():\n",
        "            metadata['line_of_therapy'] = 'second-line'\n",
        "        else:\n",
        "            metadata['line_of_therapy'] = 'later-line'\n",
        "\n",
        "        # Extract drug information\n",
        "        if 'pembrolizumab' in content.lower():\n",
        "            metadata['drug_class'] = 'anti-PD-1'\n",
        "            metadata['primary_drug'] = 'pembrolizumab'\n",
        "        elif 'nivolumab' in content.lower():\n",
        "            metadata['drug_class'] = 'anti-PD-1'\n",
        "            metadata['primary_drug'] = 'nivolumab'\n",
        "        elif 'encorafenib' in content.lower():\n",
        "            metadata['drug_class'] = 'targeted'\n",
        "            metadata['primary_drug'] = 'encorafenib'\n",
        "\n",
        "        # Extract endpoints\n",
        "        if 'overall survival' in content.lower():\n",
        "            metadata['primary_endpoint'] = 'overall_survival'\n",
        "        if 'progression-free survival' in content.lower():\n",
        "            metadata['includes_pfs'] = True\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any]):\n",
        "        \"\"\"Add document with enhanced metadata\"\"\"\n",
        "\n",
        "        # Create embedding\n",
        "        embedding = self.encoder.encode(text)\n",
        "\n",
        "        # Create document\n",
        "        doc = Document(\n",
        "            id=doc_id,\n",
        "            text=text,\n",
        "            metadata=metadata,\n",
        "            embedding=embedding\n",
        "        )\n",
        "\n",
        "        self.documents.append(doc)\n",
        "\n",
        "        # Update BM25 index\n",
        "        tokens = text.lower().split()\n",
        "        self.tokenized_docs.append(tokens)\n",
        "\n",
        "        # Rebuild indices\n",
        "        self._build_indices()\n",
        "\n",
        "    def _build_indices(self):\n",
        "        \"\"\"Build both semantic and keyword indices\"\"\"\n",
        "\n",
        "        # FAISS for semantic search\n",
        "        if self.documents:\n",
        "            embeddings = np.array([doc.embedding for doc in self.documents]).astype('float32')\n",
        "            self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "            self.index.add(embeddings)\n",
        "\n",
        "        # BM25 for keyword search\n",
        "        if self.tokenized_docs:\n",
        "            self.bm25 = BM25Okapi(self.tokenized_docs)\n",
        "\n",
        "    def hybrid_retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Hybrid retrieval: semantic + keyword search with reranking\"\"\"\n",
        "\n",
        "        # Semantic search\n",
        "        query_embedding = self.encoder.encode(query).astype('float32').reshape(1, -1)\n",
        "        semantic_scores, semantic_indices = self.index.search(query_embedding, min(top_k * 2, len(self.documents)))\n",
        "\n",
        "        # Keyword search\n",
        "        query_tokens = query.lower().split()\n",
        "        keyword_scores = self.bm25.get_scores(query_tokens)\n",
        "\n",
        "        # Combine scores (normalize and weight)\n",
        "        results = []\n",
        "        for i, doc in enumerate(self.documents):\n",
        "            semantic_score = 0\n",
        "            if i in semantic_indices[0]:\n",
        "                idx_pos = np.where(semantic_indices[0] == i)[0][0]\n",
        "                semantic_score = semantic_scores[0][idx_pos]\n",
        "\n",
        "            keyword_score = keyword_scores[i] if i < len(keyword_scores) else 0\n",
        "\n",
        "            # Weighted combination (60% semantic, 40% keyword)\n",
        "            combined_score = 0.6 * semantic_score + 0.4 * keyword_score\n",
        "\n",
        "            results.append({\n",
        "                'document': doc,\n",
        "                'semantic_score': float(semantic_score),\n",
        "                'keyword_score': float(keyword_score),\n",
        "                'combined_score': float(combined_score)\n",
        "            })\n",
        "\n",
        "        # Sort by combined score and return top-k\n",
        "        results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "    def rerank_with_llm(self, query: str, candidates: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Use LLM to rerank retrieved candidates\"\"\"\n",
        "\n",
        "        if len(candidates) <= 1:\n",
        "            return candidates\n",
        "\n",
        "        # Prepare candidates for reranking\n",
        "        candidate_texts = []\n",
        "        for i, candidate in enumerate(candidates):\n",
        "            doc = candidate['document']\n",
        "            candidate_texts.append(f\"Document {i+1}: {doc.text[:200]}...\")\n",
        "\n",
        "        rerank_prompt = f\"\"\"\n",
        "You are evaluating document relevance for a clinical research query.\n",
        "\n",
        "QUERY: {query}\n",
        "\n",
        "CANDIDATE DOCUMENTS:\n",
        "{chr(10).join(candidate_texts)}\n",
        "\n",
        "TASK: Rank these documents by relevance to the query (most relevant first).\n",
        "Consider:\n",
        "- Direct answer to the question\n",
        "- Clinical accuracy and specificity\n",
        "- Completeness of information\n",
        "\n",
        "OUTPUT: Only respond with the ranking as numbers separated by commas (e.g., \"3,1,4,2\")\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\", \"content\": rerank_prompt}],\n",
        "                max_tokens=50,\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "            # Parse ranking\n",
        "            ranking_text = response.choices[0].message.content.strip()\n",
        "            rankings = [int(x.strip()) - 1 for x in ranking_text.split(',')]\n",
        "\n",
        "            # Reorder candidates\n",
        "            reranked = [candidates[i] for i in rankings if i < len(candidates)]\n",
        "            return reranked\n",
        "\n",
        "        except:\n",
        "            # Fallback to original order if reranking fails\n",
        "            return candidates\n",
        "\n",
        "    def synthesize_answer(self, query: str, docs: List[Dict]) -> str:\n",
        "        \"\"\"Generate comprehensive answer from multiple documents\"\"\"\n",
        "\n",
        "        # Prepare context with document metadata\n",
        "        context_parts = []\n",
        "        for i, doc_result in enumerate(docs):\n",
        "            doc = doc_result['document']\n",
        "            metadata_str = \", \".join([f\"{k}: {v}\" for k, v in doc.metadata.items()])\n",
        "            context_parts.append(\n",
        "                f\"Document {i+1} ({metadata_str}):\\n{doc.text}\"\n",
        "            )\n",
        "\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a senior clinical research analyst synthesizing information from multiple trial protocols.\n",
        "\n",
        "CONTEXT DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "QUERY: {query}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Synthesize information from multiple documents when relevant\n",
        "- Compare and contrast findings across different studies if applicable\n",
        "- Highlight any contradictions or important differences\n",
        "- Provide specific citations (e.g., \"According to Document 2...\")\n",
        "- If comparing multiple trials, create a structured comparison\n",
        "\n",
        "COMPREHENSIVE ANALYSIS:\n",
        "\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=600,\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced RAG pipeline with hybrid retrieval and reranking\"\"\"\n",
        "\n",
        "        # Step 1: Hybrid retrieval\n",
        "        candidates = self.hybrid_retrieve(question, top_k=5)\n",
        "\n",
        "        if not candidates:\n",
        "            return {\n",
        "                'answer': \"No relevant documents found.\",\n",
        "                'sources': [],\n",
        "                'method': 'intermediate_rag'\n",
        "            }\n",
        "\n",
        "        # Step 2: LLM reranking\n",
        "        reranked_candidates = self.rerank_with_llm(question, candidates[:3])\n",
        "\n",
        "        # Step 3: Generate comprehensive answer\n",
        "        answer = self.synthesize_answer(question, reranked_candidates)\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': [doc['document'].metadata for doc in reranked_candidates],\n",
        "            'retrieval_scores': {\n",
        "                'semantic': [doc['semantic_score'] for doc in reranked_candidates],\n",
        "                'keyword': [doc['keyword_score'] for doc in reranked_candidates],\n",
        "                'combined': [doc['combined_score'] for doc in reranked_candidates]\n",
        "            },\n",
        "            'method': 'intermediate_rag'\n",
        "        }\n"
      ],
      "metadata": {
        "id": "_F4o8LBYQzae"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üé® GRADIO INTERFACE WRAPPER\n",
        ""
      ],
      "metadata": {
        "id": "LNj9AUAVQ275"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGWorkshopInterface:\n",
        "    \"\"\"\n",
        "    Gradio interface wrapper for the RAG workshop.\n",
        "    Preserves ALL original functionality while adding beautiful visual interface.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.basic_rag = None\n",
        "        self.intermediate_rag = None\n",
        "        self.api_key = None\n",
        "        self.setup_status = {\n",
        "            'basic': False,\n",
        "            'intermediate': False\n",
        "        }\n",
        "\n",
        "    def setup_basic_rag(self, api_key: str) -> str:\n",
        "        \"\"\"Setup Basic RAG system\"\"\"\n",
        "        try:\n",
        "            if not api_key.strip():\n",
        "                return \"‚ùå Please enter your OpenAI API key\"\n",
        "\n",
        "            self.api_key = api_key\n",
        "            self.basic_rag = BasicRAG(api_key)\n",
        "\n",
        "            # Load sample clinical trial documents\n",
        "            trial_docs = [\n",
        "                {\n",
        "                    'text': \"\"\"\n",
        "Phase III randomized, double-blind, placebo-controlled study of Drug X\n",
        "in patients with metastatic colorectal cancer. Primary endpoint is\n",
        "overall survival. Secondary endpoints include progression-free survival\n",
        "and objective response rate. Study population: 600 patients with\n",
        "confirmed metastatic colorectal cancer, ECOG performance status 0-1,\n",
        "adequate organ function. Exclusion criteria include prior anti-VEGF\n",
        "therapy, uncontrolled hypertension, recent surgery within 28 days.\n",
        "                    \"\"\".strip(),\n",
        "                    'metadata': {'study_id': 'CRC-301', 'phase': 'III', 'indication': 'Colorectal Cancer'}\n",
        "                },\n",
        "                {\n",
        "                    'text': \"\"\"\n",
        "Inclusion criteria for the colorectal cancer study: Age ‚â•18 years,\n",
        "histologically confirmed adenocarcinoma of colon or rectum, measurable\n",
        "disease per RECIST 1.1, life expectancy >3 months, signed informed\n",
        "consent. Laboratory requirements: ANC ‚â•1500/ŒºL, platelets ‚â•100,000/ŒºL,\n",
        "hemoglobin ‚â•9 g/dL, total bilirubin ‚â§1.5√ó ULN, AST/ALT ‚â§2.5√ó ULN.\n",
        "                    \"\"\".strip(),\n",
        "                    'metadata': {'study_id': 'CRC-301', 'section': 'Inclusion Criteria'}\n",
        "                },\n",
        "                {\n",
        "                    'text': \"\"\"\n",
        "Drug X dosing regimen: 10mg/kg IV every 2 weeks until disease\n",
        "progression or unacceptable toxicity. Dose modifications allowed for\n",
        "grade 3+ toxicities. Concomitant medications: Standard supportive care\n",
        "allowed, no other anticancer therapy permitted during study treatment.\n",
        "Regular safety monitoring with lab assessments every 2 weeks, imaging\n",
        "every 6 weeks per RECIST 1.1.\n",
        "                    \"\"\".strip(),\n",
        "                    'metadata': {'study_id': 'CRC-301', 'section': 'Treatment Protocol'}\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Add documents to knowledge base\n",
        "            for doc in trial_docs:\n",
        "                self.basic_rag.add_document(doc['text'], doc['metadata'])\n",
        "\n",
        "            self.setup_status['basic'] = True\n",
        "            return f\"‚úÖ Basic RAG initialized successfully!\\nüìö Loaded {len(trial_docs)} clinical trial documents\\nü§ñ Using HuggingFace sentence-transformers model\\nüóÑÔ∏è FAISS vector database ready\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error setting up Basic RAG: {str(e)}\"\n",
        "\n",
        "    def setup_intermediate_rag(self, api_key: str) -> str:\n",
        "        \"\"\"Setup Intermediate RAG system\"\"\"\n",
        "        try:\n",
        "            if not api_key.strip():\n",
        "                return \"‚ùå Please enter your OpenAI API key\"\n",
        "\n",
        "            self.api_key = api_key\n",
        "            self.intermediate_rag = IntermediateRAG(api_key)\n",
        "\n",
        "            # Create and load sample PDF files\n",
        "            pdf_files = self.intermediate_rag.create_sample_pdfs()\n",
        "            self.intermediate_rag.load_pdf_documents(pdf_files)\n",
        "\n",
        "            studies = set([doc.metadata.get('study', 'Unknown') for doc in self.intermediate_rag.documents])\n",
        "\n",
        "            self.setup_status['intermediate'] = True\n",
        "            return f\"‚úÖ Intermediate RAG initialized successfully!\\nüìö Loaded {len(self.intermediate_rag.documents)} document chunks from {len(pdf_files)} clinical protocols\\nüéØ Available studies: {', '.join(studies)}\\nüîç Hybrid search (semantic + keyword) ready\\nü§ñ LLM reranking enabled\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error setting up Intermediate RAG: {str(e)}\"\n",
        "\n",
        "    def query_basic_rag(self, question: str) -> Tuple[str, str, str]:\n",
        "        \"\"\"Query Basic RAG system\"\"\"\n",
        "        if not self.setup_status['basic']:\n",
        "            return \"‚ùå Please setup Basic RAG first\", \"\", \"\"\n",
        "\n",
        "        if not question.strip():\n",
        "            return \"‚ùå Please enter a question\", \"\", \"\"\n",
        "\n",
        "        try:\n",
        "            result = self.basic_rag.query(question)\n",
        "\n",
        "            # Format answer\n",
        "            answer = result['answer']\n",
        "\n",
        "            # Format sources\n",
        "            sources_info = []\n",
        "            for i, (source, score) in enumerate(zip(result['sources'], result['scores'])):\n",
        "                study_id = source.get('study_id', 'Unknown')\n",
        "                section = source.get('section', 'Main Content')\n",
        "                sources_info.append(f\"üìÑ **Source {i+1}:** {study_id} - {section} (Relevance: {score:.3f})\")\n",
        "\n",
        "            sources_display = \"\\n\".join(sources_info)\n",
        "\n",
        "            # Format technical details\n",
        "            tech_details = f\"\"\"**üîß Technical Details:**\n",
        "- **Method:** {result['method']}\n",
        "- **Retrieved Documents:** {len(result['sources'])}\n",
        "- **Embedding Model:** sentence-transformers/all-MiniLM-L6-v2\n",
        "- **Vector Database:** FAISS IndexFlatIP\n",
        "- **Similarity Metric:** Cosine Similarity\"\"\"\n",
        "\n",
        "            return answer, sources_display, tech_details\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error querying Basic RAG: {str(e)}\", \"\", \"\"\n",
        "\n",
        "    def query_intermediate_rag(self, question: str) -> Tuple[str, str, str]:\n",
        "        \"\"\"Query Intermediate RAG system\"\"\"\n",
        "        if not self.setup_status['intermediate']:\n",
        "            return \"‚ùå Please setup Intermediate RAG first\", \"\", \"\"\n",
        "\n",
        "        if not question.strip():\n",
        "            return \"‚ùå Please enter a question\", \"\", \"\"\n",
        "\n",
        "        try:\n",
        "            result = self.intermediate_rag.query(question)\n",
        "\n",
        "            # Format answer\n",
        "            answer = result['answer']\n",
        "\n",
        "            # Format sources with enhanced metadata\n",
        "            sources_info = []\n",
        "            for i, source in enumerate(result['sources']):\n",
        "                study = source.get('study', 'Unknown')\n",
        "                indication = source.get('indication', 'Unknown')\n",
        "                chunk_num = source.get('chunk_number', '?')\n",
        "\n",
        "                # Get scores\n",
        "                semantic_score = result['retrieval_scores']['semantic'][i]\n",
        "                keyword_score = result['retrieval_scores']['keyword'][i]\n",
        "                combined_score = result['retrieval_scores']['combined'][i]\n",
        "\n",
        "                sources_info.append(f\"\"\"üìÑ **Source {i+1}:** {study} - {indication}\n",
        "   - **Chunk:** {chunk_num}\n",
        "   - **Semantic Score:** {semantic_score:.3f}\n",
        "   - **Keyword Score:** {keyword_score:.3f}\n",
        "   - **Combined Score:** {combined_score:.3f}\"\"\")\n",
        "\n",
        "            sources_display = \"\\n\\n\".join(sources_info)\n",
        "\n",
        "            # Format technical details\n",
        "            tech_details = f\"\"\"**üîß Technical Details:**\n",
        "- **Method:** {result['method']} (Hybrid Search + LLM Reranking)\n",
        "- **Retrieved Documents:** {len(result['sources'])}\n",
        "- **Search Strategy:** 60% Semantic + 40% Keyword (BM25)\n",
        "- **Embedding Model:** sentence-transformers/all-MiniLM-L6-v2\n",
        "- **Vector Database:** FAISS IndexFlatIP\n",
        "- **Reranking:** GPT-4o-mini\n",
        "- **Total Indexed Chunks:** {len(self.intermediate_rag.documents)}\"\"\"\n",
        "\n",
        "            return answer, sources_display, tech_details\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error querying Intermediate RAG: {str(e)}\", \"\", \"\"\n",
        "\n",
        "    def create_interface(self):\n",
        "        \"\"\"Create the Gradio interface\"\"\"\n",
        "\n",
        "        with gr.Blocks(\n",
        "            title=\"üß† RAG Workshop - Kognitic AI for Engineers\",\n",
        "            theme=gr.themes.Soft(),\n",
        "            css=\"\"\"\n",
        "            .gradio-container {\n",
        "                max-width: 1200px !important;\n",
        "            }\n",
        "            .tab-nav {\n",
        "                background: linear-gradient(90deg, #667eea 0%, #764ba2 100%) !important;\n",
        "            }\n",
        "            \"\"\"\n",
        "        ) as interface:\n",
        "\n",
        "            # Header\n",
        "            gr.Markdown(\"\"\"\n",
        "            # üß† RAG (Retrieval-Augmented Generation) Workshop\n",
        "            ## Kognitic AI for Engineers - Progressive Examples\n",
        "\n",
        "            **üéØ Learning Objectives:**\n",
        "            - ‚úÖ Understand RAG architecture and components\n",
        "            - ‚úÖ See impact of chunking strategies and embedding models\n",
        "            - ‚úÖ Learn hybrid search techniques (semantic + keyword)\n",
        "            - ‚úÖ Experience reranking for improved precision\n",
        "            - ‚úÖ Apply RAG to real clinical research problems\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Tabs() as tabs:\n",
        "\n",
        "                # ==================== BASIC RAG TAB ====================\n",
        "                with gr.TabItem(\"üìö Basic RAG\", id=\"basic_rag\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## üìö Basic RAG - Clinical Trial Protocol Q&A\n",
        "\n",
        "                    **üéØ Educational Focus:**\n",
        "                    - Basic RAG pipeline: Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate\n",
        "                    - Using HuggingFace embedding models\n",
        "                    - Simple vector similarity search with FAISS\n",
        "                    \"\"\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=2):\n",
        "                            basic_api_key = gr.Textbox(\n",
        "                                label=\"üîë OpenAI API Key\",\n",
        "                                type=\"password\",\n",
        "                                placeholder=\"Enter your OpenAI API key...\",\n",
        "                                info=\"Your API key is used only for this session and not stored.\"\n",
        "                            )\n",
        "                            basic_setup_btn = gr.Button(\"üöÄ Setup Basic RAG\", variant=\"primary\")\n",
        "                            basic_setup_status = gr.Textbox(\n",
        "                                label=\"üìä Setup Status\",\n",
        "                                value=\"‚è≥ Click 'Setup Basic RAG' to initialize...\",\n",
        "                                interactive=False,\n",
        "                                lines=4\n",
        "                            )\n",
        "\n",
        "                        with gr.Column(scale=3):\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            **üìã Sample Documents Included:**\n",
        "                            - üß¨ **CRC-301:** Phase III colorectal cancer study\n",
        "                            - üìë **Inclusion Criteria:** Patient eligibility requirements\n",
        "                            - üíä **Treatment Protocol:** Drug dosing and monitoring\n",
        "\n",
        "                            **üîß Technical Stack:**\n",
        "                            - **Embeddings:** sentence-transformers/all-MiniLM-L6-v2\n",
        "                            - **Vector DB:** FAISS (IndexFlatIP)\n",
        "                            - **Chunking:** Sentence-based with overlap\n",
        "                            - **Similarity:** Cosine similarity\n",
        "                            \"\"\")\n",
        "\n",
        "                    gr.Markdown(\"---\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            basic_question = gr.Textbox(\n",
        "                                label=\"‚ùì Your Clinical Research Question\",\n",
        "                                placeholder=\"e.g., What is the primary endpoint of the colorectal cancer study?\",\n",
        "                                lines=2\n",
        "                            )\n",
        "                            basic_query_btn = gr.Button(\"üîç Ask Basic RAG\", variant=\"secondary\")\n",
        "\n",
        "                        with gr.Column():\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            **üí° Try These Example Questions:**\n",
        "                            - What is the primary endpoint of the colorectal cancer study?\n",
        "                            - What are the dosing requirements for Drug X?\n",
        "                            - What laboratory values are required for patient eligibility?\n",
        "                            - What are the exclusion criteria for the study?\n",
        "                            \"\"\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=2):\n",
        "                            basic_answer = gr.Textbox(\n",
        "                                label=\"üí° RAG Answer\",\n",
        "                                lines=8,\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            basic_sources = gr.Textbox(\n",
        "                                label=\"üìö Retrieved Sources\",\n",
        "                                lines=6,\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                    basic_tech_details = gr.Textbox(\n",
        "                        label=\"üîß Technical Details\",\n",
        "                        lines=4,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                # ==================== INTERMEDIATE RAG TAB ====================\n",
        "                with gr.TabItem(\"üîç Intermediate RAG\", id=\"intermediate_rag\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ## üîç Intermediate RAG - Multi-Document Analysis with Reranking\n",
        "\n",
        "                    **üéØ Educational Focus:**\n",
        "                    - Multi-document PDF processing and cross-document synthesis\n",
        "                    - Hybrid search: semantic (FAISS) + keyword (BM25)\n",
        "                    - LLM reranking for improved precision\n",
        "                    - Rich metadata extraction and filtering\n",
        "                    \"\"\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=2):\n",
        "                            inter_api_key = gr.Textbox(\n",
        "                                label=\"üîë OpenAI API Key\",\n",
        "                                type=\"password\",\n",
        "                                placeholder=\"Enter your OpenAI API key...\",\n",
        "                                info=\"Your API key is used only for this session and not stored.\"\n",
        "                            )\n",
        "                            inter_setup_btn = gr.Button(\"üöÄ Setup Intermediate RAG\", variant=\"primary\")\n",
        "                            inter_setup_status = gr.Textbox(\n",
        "                                label=\"üìä Setup Status\",\n",
        "                                value=\"‚è≥ Click 'Setup Intermediate RAG' to initialize...\",\n",
        "                                interactive=False,\n",
        "                                lines=6\n",
        "                            )\n",
        "\n",
        "                        with gr.Column(scale=3):\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            **üìã Clinical Trial Protocols Included:**\n",
        "                            - ü´Å **KEYNOTE-189:** Pembrolizumab + Chemo in NSCLC\n",
        "                            - ü´Å **CheckMate-227:** Nivolumab + Ipilimumab in NSCLC\n",
        "                            - üéØ **BEACON CRC:** BRAF-targeted therapy in colorectal cancer\n",
        "\n",
        "                            **üîß Advanced Technical Stack:**\n",
        "                            - **Hybrid Search:** 60% Semantic + 40% Keyword (BM25)\n",
        "                            - **Reranking:** GPT-4o-mini for precision improvement\n",
        "                            - **Chunking:** Advanced with overlap and metadata\n",
        "                            - **Cross-Document:** Multi-study synthesis and comparison\n",
        "                            \"\"\")\n",
        "\n",
        "                    gr.Markdown(\"---\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            inter_question = gr.Textbox(\n",
        "                                label=\"‚ùì Your Cross-Document Research Question\",\n",
        "                                placeholder=\"e.g., Compare the overall survival results across all three studies\",\n",
        "                                lines=3\n",
        "                            )\n",
        "                            inter_query_btn = gr.Button(\"üîç Ask Intermediate RAG\", variant=\"secondary\")\n",
        "\n",
        "                        with gr.Column():\n",
        "                            gr.Markdown(\"\"\"\n",
        "                            **üí° Try These Cross-Document Questions:**\n",
        "                            - Compare the overall survival results across all three studies\n",
        "                            - What are the different biomarker strategies used in lung cancer vs colorectal cancer studies?\n",
        "                            - How do the safety profiles differ between the lung cancer immunotherapy studies?\n",
        "                            - Which study shows the best objective response rate and why?\n",
        "                            - What are the key differences in patient eligibility criteria across these trials?\n",
        "                            \"\"\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=2):\n",
        "                            inter_answer = gr.Textbox(\n",
        "                                label=\"üí° Synthesized Cross-Document Answer\",\n",
        "                                lines=10,\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                        with gr.Column(scale=1):\n",
        "                            inter_sources = gr.Textbox(\n",
        "                                label=\"üìö Retrieved Sources with Scores\",\n",
        "                                lines=8,\n",
        "                                interactive=False\n",
        "                            )\n",
        "\n",
        "                    inter_tech_details = gr.Textbox(\n",
        "                        label=\"üîß Advanced Technical Details\",\n",
        "                        lines=5,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                # ==================== EDUCATIONAL NOTES TAB ====================\n",
        "                with gr.TabItem(\"üéì Educational Notes\", id=\"educational\"):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    # üéì RAG Workshop - Educational Notes\n",
        "\n",
        "                    ## üèóÔ∏è RAG Architecture Overview\n",
        "\n",
        "                    ```\n",
        "                    üìù Documents ‚Üí üî™ Chunking ‚Üí ü§ñ Embedding ‚Üí üóÑÔ∏è Vector DB\n",
        "                                                                      ‚Üì\n",
        "                    üí° Answer ‚Üê üß† LLM Generation ‚Üê üìö Context ‚Üê üîç Retrieval ‚Üê ‚ùì Query\n",
        "                    ```\n",
        "\n",
        "                    ## üìö Basic RAG vs üîç Intermediate RAG\n",
        "\n",
        "                    | Feature | Basic RAG | Intermediate RAG |\n",
        "                    |---------|-----------|------------------|\n",
        "                    | **Search Method** | Semantic only (FAISS) | Hybrid (Semantic + Keyword) |\n",
        "                    | **Documents** | 3 simple chunks | Multiple PDFs with metadata |\n",
        "                    | **Reranking** | None | LLM-based reranking |\n",
        "                    | **Cross-Document** | Single document focus | Multi-document synthesis |\n",
        "                    | **Metadata** | Basic | Rich extraction + filtering |\n",
        "                    | **Complexity** | Educational baseline | Production-ready approach |\n",
        "\n",
        "                    ## üîß Technical Implementation Details\n",
        "\n",
        "                    ### ü§ñ Embedding Models\n",
        "                    - **Model:** `sentence-transformers/all-MiniLM-L6-v2`\n",
        "                    - **Dimensions:** 384-dimensional embeddings\n",
        "                    - **Speed:** Fast inference, good for prototyping\n",
        "                    - **Quality:** Balanced performance across domains\n",
        "\n",
        "                    ### üóÑÔ∏è Vector Databases\n",
        "                    - **FAISS:** High-performance similarity search\n",
        "                    - **IndexFlatIP:** Inner product for cosine similarity\n",
        "                    - **Normalization:** L2 normalization for proper cosine similarity\n",
        "\n",
        "                    ### üîç Hybrid Search Strategy\n",
        "                    - **Semantic Search:** Captures meaning and context\n",
        "                    - **Keyword Search (BM25):** Captures exact term matches\n",
        "                    - **Combination:** 60% semantic + 40% keyword weighting\n",
        "                    - **Benefits:** Better recall and precision balance\n",
        "\n",
        "                    ### üéØ Reranking Benefits\n",
        "                    - **Purpose:** Improve precision of top results\n",
        "                    - **Method:** LLM evaluates relevance to specific query\n",
        "                    - **Trade-off:** Higher accuracy vs increased latency\n",
        "                    - **When to Use:** Complex queries requiring nuanced understanding\n",
        "\n",
        "                    ## üöÄ Production Considerations\n",
        "\n",
        "                    ### üìä Evaluation Metrics\n",
        "                    - **Retrieval Quality:** Precision@K, Recall@K, MRR\n",
        "                    - **Answer Quality:** Human evaluation, factual accuracy\n",
        "                    - **End-to-End:** Response relevance, user satisfaction\n",
        "\n",
        "                    ### ‚ö° Performance Optimization\n",
        "                    - **Chunking Strategy:** Balance context vs granularity\n",
        "                    - **Index Type:** HNSW for large scale, IVF for memory efficiency\n",
        "                    - **Caching:** Query results and embeddings\n",
        "                    - **Batch Processing:** Bulk document ingestion\n",
        "\n",
        "                    ### üè≠ Scaling to Production\n",
        "                    1. **Start Simple:** Basic RAG with good chunking\n",
        "                    2. **Add Hybrid Search:** Improve recall with keyword matching\n",
        "                    3. **Implement Reranking:** Boost precision for complex queries\n",
        "                    4. **Rich Metadata:** Enable filtering and boosting\n",
        "                    5. **Monitor & Evaluate:** Continuous improvement cycle\n",
        "\n",
        "                    ## üî¨ Advanced RAG Techniques (Beyond This Workshop)\n",
        "\n",
        "                    ### üéØ Query Enhancement\n",
        "                    - **Query Expansion:** Add related terms automatically\n",
        "                    - **Query Rewriting:** Rephrase for better retrieval\n",
        "                    - **Multi-Query:** Generate multiple query variants\n",
        "\n",
        "                    ### üìà Advanced Retrieval\n",
        "                    - **Two-Stage Retrieval:** Fast first-pass + precise reranking\n",
        "                    - **Dense + Sparse:** ColBERT, SPLADE for better precision\n",
        "                    - **Graph RAG:** Knowledge graph integration\n",
        "\n",
        "                    ### üß† Generation Enhancement\n",
        "                    - **Chain-of-Thought:** Step-by-step reasoning\n",
        "                    - **Self-RAG:** Model decides when to retrieve\n",
        "                    - **Adaptive RAG:** Route queries to different strategies\n",
        "\n",
        "                    ## üìö Recommended Resources\n",
        "\n",
        "                    - **HuggingFace Sentence Transformers:** https://huggingface.co/sentence-transformers\n",
        "                    - **FAISS Documentation:** https://faiss.ai/\n",
        "                    - **ChromaDB (Production Vector DB):** https://docs.trychroma.com/\n",
        "                    - **LangChain RAG Guide:** https://python.langchain.com/docs/use_cases/question_answering\n",
        "                    - **Pinecone RAG Handbook:** https://www.pinecone.io/learn/retrieval-augmented-generation/\n",
        "                    \"\"\")\n",
        "\n",
        "            # Event handlers\n",
        "            basic_setup_btn.click(\n",
        "                self.setup_basic_rag,\n",
        "                inputs=[basic_api_key],\n",
        "                outputs=[basic_setup_status]\n",
        "            )\n",
        "\n",
        "            inter_setup_btn.click(\n",
        "                self.setup_intermediate_rag,\n",
        "                inputs=[inter_api_key],\n",
        "                outputs=[inter_setup_status]\n",
        "            )\n",
        "\n",
        "            basic_query_btn.click(\n",
        "                self.query_basic_rag,\n",
        "                inputs=[basic_question],\n",
        "                outputs=[basic_answer, basic_sources, basic_tech_details]\n",
        "            )\n",
        "\n",
        "            inter_query_btn.click(\n",
        "                self.query_intermediate_rag,\n",
        "                inputs=[inter_question],\n",
        "                outputs=[inter_answer, inter_sources, inter_tech_details]\n",
        "            )\n",
        "\n",
        "        return interface\n"
      ],
      "metadata": {
        "id": "CfwGzitIRFhM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ MAIN RUNNER"
      ],
      "metadata": {
        "id": "gmO7iC4TRIto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "wKBFom8uZxwc",
        "outputId": "e3d79c07-0a72-47fc-d5bd-fe521a16aa0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† RAG Workshop - Launching Gradio Interface...\n",
            "============================================================\n",
            "üé® Gradio interface created successfully!\n",
            "üöÄ Launching workshop...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a485ebf48383b2e372.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a485ebf48383b2e372.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def launch_rag_workshop():\n",
        "    \"\"\"Launch the RAG workshop with Gradio interface\"\"\"\n",
        "\n",
        "    print(\"üß† RAG Workshop - Launching Gradio Interface...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create interface\n",
        "    workshop = RAGWorkshopInterface()\n",
        "    interface = workshop.create_interface()\n",
        "\n",
        "    print(\"üé® Gradio interface created successfully!\")\n",
        "    print(\"üöÄ Launching workshop...\")\n",
        "\n",
        "    # Launch interface\n",
        "    interface.launch(\n",
        "        share=True,  # Create public link for Colab\n",
        "        server_name=\"0.0.0.0\",  # Allow external access\n",
        "        server_port=7860,  # Standard Gradio port\n",
        "        show_api=False,  # Hide API docs for cleaner interface\n",
        "        favicon_path=None,  # Use default favicon\n",
        "        inbrowser=True  # Auto-open in browser\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launch_rag_workshop()"
      ]
    }
  ]
}